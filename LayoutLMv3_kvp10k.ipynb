{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/667029/KVP10k/blob/main/LayoutLMv3_kvp10k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sitering for å bruke Modellen\n",
        "\n",
        "```\n",
        "@inproceedings{huang2022layoutlmv3,\n",
        "  author={Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},\n",
        "  title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},\n",
        "  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},\n",
        "  year={2022}\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "_ri7o_DyrGgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os              #navigere mapper og filer, hente filbaner\n",
        "from PIL import Image  #åpne, vise og manipulere bilder\n",
        "import json            #lese/skrive til JSON-filer\n",
        "\n",
        "\n",
        "#Installer nødvendige biblioteker\n",
        "!pip install -q Pillow\n",
        "!pip install -q transformers datasets torch torchvision"
      ],
      "metadata": {
        "id": "pCNtevUl6Lc4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "53oEUaCEnV5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69a113f-1fb0-4963-b29b-2c3d42d7899e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/DAT255/KVP10k-dataset/train\"\n",
        "print(os.listdir(base_path))"
      ],
      "metadata": {
        "id": "sG03nbg7VELk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59f4226d-5e24-438b-b245-3b93ce91a2b7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ocrs', 'gts', 'images', 'items']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LayoutLMv3 - Hva skal jeg bruke?"
      ],
      "metadata": {
        "id": "5FA-ID2DJ6TN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Steg 1. Må utføre Entity Extraction (EE).\n",
        "\n",
        "- Steg 2. Utføre Relation Exctraction (RE). Formaliser RE som et matrise-problem, hvor man for alle par av entiteter **predikerer** om en relasjon finnes.\n",
        "  - Relasjoner kan representeres som en N x N matrise over entitets-paret (i, j).\n",
        "\n",
        "- Bruk en enkelt treningspipeline.\n",
        "\n",
        "- Bruke **LayoutLMv3Processor** for å kombinere bilde, tekst, og bboxes, for å lage den input modellen krever.\n",
        "\n",
        "- Bruke **LayoutLMv3ForTokenClassification** som modell for per-token tagging (EE).\n",
        "  - Denne brukes for Entity tagging.\n",
        "  - UTVID MED --> Ekstra kode for \"*span grouping*\" og \"*entity embedding*\".\n",
        "\n",
        "_____\n",
        "\n",
        "Steg 1:\n",
        "  - Tagge tokens med (O, B-KEY, B-VALUE, I-KEY, I-VALUE), og derretter koble KEY --> VALUE.\n",
        "\n",
        "Label-mappinger:\n",
        "\n",
        "\n",
        "```\n",
        "label2id = {\n",
        "    \"O\": 0,\n",
        "    \"B-KEY\": 1,\n",
        "    \"I-KEY\": 2,\n",
        "    \"B-VALUE\": 3,\n",
        "    \"I-VALUE\": 4,\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Steg 2:\n",
        "LayoutLMv3Model er grunnlag for RE.\n",
        "For å bruke denne modellen som egentlig er PyTorch Native, må vi bruke installere *transformers* og bruke PyTorch-backend.\n",
        "UTVID med --> Egen relasjonsklassifisering mellom *key* og *value* spans\n",
        "  - Kobling mellom key og value:\n",
        "    1. Hente predikerte spans fra EE\n",
        "    2. Lage relasjonsprediksjonsmodell RE\n",
        "\n",
        "*Hentet fra:* https://arxiv.org/html/2404.10848v1\n"
      ],
      "metadata": {
        "id": "f5Y1LTywKg4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Oppsummert\n",
        "\n",
        "- Bruke LayoutLMv3Processor for å kombinere bilde, tokens, og bounding boxes (bboxes).\n",
        "\n",
        "- Bruke LayoutLMv3ForTokenClassification til å tagge tokens som B-KEY, B-VALYE, O, etc.\n",
        "\n",
        "\n",
        "1. Må utvide LayoutLMv3Model til å hente embeddings.\n",
        "2. Implementere RE ved å sammenligne KVPs.\n",
        "3. Evaluer RE med F1/Precision/Recall per relasjon."
      ],
      "metadata": {
        "id": "Si04z3FqTSTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch (+ Hugging Face)\n",
        "LayoutLMv3 er en del av *transformers*-biblioteket til Hugging Face.\n",
        "Dette betyr at enten så må hele pipelinen utvikles i PyTorch, eller utvikle bare EE i PyTorch og få KEY-VALUE-Spans, så brukes TensorFlow for resten.\n",
        "\n",
        "- Siden jeg er generelt usikker på begge bibliotekene så får vi se hva som blir valgt etterhvert...\n"
      ],
      "metadata": {
        "id": "Ygiu9tTWekZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset - forståelse\n",
        "\n",
        "**Innhold i train/-mappen i KVP10k:**\n",
        "_____\n",
        "  - *images*/ --> .png bilder av hvert dokument. Visuell input for modellen.\n",
        "    - Det modellen \"ser\".\n",
        "_____\n",
        "\n",
        "  - *ocrs*/ --> JSON-filer med **words** og **bboxes** for hvert dokument. Gir tekst og posisjoner fra OCR, og brukes sammen med images.\n",
        "    - Det modellen \"leser\" (tokens og posisjonene deres).\n",
        "\n",
        "_____\n",
        "\n",
        "  - *gts*/ --> JSON-filer med KVPs og tilhørende bboxes. Inneholder hvilke keys og values som hører sammen.\n",
        "    - Det som lærer modellen hvilke tokens som er nøkler, verdier, og hvilket som er koblet sammen.\n",
        "_____\n",
        "\n",
        "  - *items*/ --> JSON-filer med annotasjoner og layout-objs (rektangler, linker, etiketter)\n",
        "    - tilleggsinformasjon\n",
        "_____"
      ],
      "metadata": {
        "id": "GHspumq8r7Xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEG 1 - Entity Exctraction"
      ],
      "metadata": {
        "id": "BclQA7plg6nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprosesser et datapunkt.\n",
        "Lese inn:\n",
        "  - OCR-data (tekst + bbox)\n",
        "  - Ground Truth (gts, key/value + bbox)\n",
        "  - Dokumentbilde (.png)\n",
        "\n",
        "\n",
        "**Det brukes BIO-tagger, og dette er hva det står for:**\n",
        " - B --> Begin: første token i en entitet.\n",
        " - I --> Inside: inne i en entitet.\n",
        " - O --> Outside: tokenen er ikke en del av noen entitet\n",
        "\n",
        "f.eks.\n",
        "  - Tokens:  [\"Name\", \"of\", \"buyer\", \":\", \"Ole\", \"Martin\", \"Lystadmoen\"]\n",
        "  - Labels:  [\"B-KEY\", \"I-KEY\", \"I-KEY\", \"O\", \"B-VALUE\", \"I-VALUE\", \"I-VALUE\"]"
      ],
      "metadata": {
        "id": "o3zE07qbqnDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LayoutLMv3Processor\n",
        "import torch\n",
        "\n",
        "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False) # <-- Viktig fordi vi allerede har utført OCR på bildet og har tekst og bboxes"
      ],
      "metadata": {
        "id": "XoaEcIXlhBwY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping fra tekstlige BIO-labels til tall som modellen bruker\n",
        "label_map = {\n",
        "    \"O\": 0,\n",
        "    \"B-KEY\": 1,\n",
        "    \"I-KEY\": 2,\n",
        "    \"B-VALUE\": 3,\n",
        "    \"I-VALUE\": 4,\n",
        "}\n",
        "\n",
        "# Funksjon for å skalere bounding boxes til 0-1000 (som LayoutLMv3 krever)\n",
        "def normalize_bbox(bbox, width, height):\n",
        "  return [\n",
        "      int(1000 * (bbox[0] /width)),\n",
        "      int(1000 * (bbox[1] / height)),\n",
        "      int(1000 * (bbox[2] / width)),\n",
        "      int(1000 * (bbox[3] / height))\n",
        "  ]\n",
        "\n",
        "\n",
        "def assign_label_for_box(box, boxes, label_type):\n",
        "  \"\"\"Returnerer liste med (index, label) for tokens osm overlapper box\"\"\"\n",
        "  overlaps = []\n",
        "  for i, token_box in enumerate(boxes):\n",
        "    if box_overlap(box, token_box) > 0:\n",
        "      overlaps.append(i)\n",
        "\n",
        "  overlaps = sorted(overlaps)\n",
        "\n",
        "  labeled = []\n",
        "  for j, idx in enumerate(overlaps):\n",
        "    tag = f\"B-{label_type}\" if j == 0 else f\"I-{label_type}\"\n",
        "    labeled.append((idx, tag))\n",
        "\n",
        "  return labeled\n",
        "\n",
        "\n",
        " #Sjekker om OCR-boksen overlapper med GTS(key/value)-boksen.\n",
        "#Ved overlapp hører de til hverandre.\n",
        "def box_overlap(box1, box2):\n",
        "  x0 = max(box1[0], box2[0])\n",
        "  y0 = max(box1[1], box2[1])\n",
        "  x1 = min(box1[2], box2[2])\n",
        "  y1 = min(box1[3], box2[3])\n",
        "  return max(0, x1 - x0) * max(0, y1 - y0)\n",
        "\n",
        "\n",
        "# Funksjon for å generere BIO-labels fra gts (ground truth).\n",
        "# Lager en BIO-label for hvert token basert på om det overlapper med en key- eller value-boks fra GTS.\n",
        "# Matcher hvert token fra OCR (word + bbox) mot key/value-bbokser fra gts:\n",
        "# --> Token overlapper en nøkkelboks: B-KEY eller I-KEY\n",
        "# --> Token overlapper en verdiboks: B-VALUE eller I-VALUE\n",
        "# --> Ellers: O\n",
        "def iob_from_kvps(words, boxes, kvps):\n",
        "  labels = [\"O\"] * len(words)\n",
        "\n",
        "  #Gå igjennom alle key-value-pairs\n",
        "  for kvp in kvps:\n",
        "    if \"key\" in kvp and \"bbox\" in kvp[\"key\"]:\n",
        "      key_bbox = kvp[\"key\"][\"bbox\"]\n",
        "      for idx, tag in assign_label_for_box(key_bbox, boxes, \"KEY\"):\n",
        "        labels[idx] = tag\n",
        "\n",
        "    if \"value\" in kvp and \"bbox\" in kvp[\"value\"]:\n",
        "      value_box = kvp[\"value\"][\"bbox\"]\n",
        "      for idx, tag in assign_label_for_box(value_box, boxes, \"VALUE\"):\n",
        "        labels[idx] = tag\n",
        "\n",
        "  return labels"
      ],
      "metadata": {
        "id": "uJl6cAPtMnQS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_example(doc_id, base_path):\n",
        "  image_path = os.path.join(base_path, \"images\", f\"{doc_id}.png\")\n",
        "  ocr_path = os.path.join(base_path, \"ocrs\", f\"{doc_id}.json\")\n",
        "  gt_path = os.path.join(base_path, \"gts\", f\"{doc_id}.json\")\n",
        "\n",
        "  image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "  with open(ocr_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    ocr_data = json.load(f)\n",
        "\n",
        "  with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    gt_data = json.load(f)\n",
        "\n",
        "  page = ocr_data[\"pages\"][0]\n",
        "  width, height = page[\"width\"], page[\"height\"]\n",
        "\n",
        "  words = [w[\"text\"] for w in page[\"words\"]]\n",
        "  raw_boxes = [w[\"bbox\"] for w in page[\"words\"]]\n",
        "  boxes = [normalize_bbox(b, width, height) for b in raw_boxes]\n",
        "\n",
        "  kvps = gt_data[\"kvps_list\"]\n",
        "  string_labels = iob_from_kvps(words, raw_boxes, kvps)\n",
        "  labels = [label_map[l] for l in string_labels]\n",
        "\n",
        "  #Fyller på med padding, og truncation klipper av hvis sekvensen har for mange tokens, returnerer som PyTorch-tensor\n",
        "  encoding = processor(image, words, boxes=boxes, word_labels=labels, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  return encoding, words, boxes, string_labels"
      ],
      "metadata": {
        "id": "oW6bF5fQrR6x"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_id = \"aaf8db8517856054da0210f56f97e0acb910ca9a96be8d295050b3c9990ff8ed\" #Eksempel dok. fra KVP10k\n",
        "encoding, words, boxes, tags = load_example(doc_id, base_path)\n",
        "\n",
        "print(encoding.keys())\n",
        "\n",
        "for w, t in zip(words, tags):\n",
        "  print(f\"{w}: {t}\")"
      ],
      "metadata": {
        "id": "H_BQwlWGveFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grunnen til at det er så mye \"O\"-tagger er fordi det typisk i slike dokument-annotasjonsdatasett så er andelen tokens som er relevant (KEY/VAL) ofte kun 5-15% av ALLE tokens."
      ],
      "metadata": {
        "id": "gVQnDjFeg3-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hva er gjort så langt?\n",
        "\n",
        "- Lest inn images, ocrs, gts\n",
        "- Laget BIO-tagger\n",
        "- Verifisert at tokens for korrekte tagger\n",
        "- En fungerende load_example()"
      ],
      "metadata": {
        "id": "SN6P9-NnksB6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJBmaskUlOV_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}