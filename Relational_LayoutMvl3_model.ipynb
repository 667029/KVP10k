{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Dokumentforståelse med LayoutLMv3 på KVP10k-datasettet\n",
    "\n",
    "Denne notebooken demonstrerer hvordan vi henter inn en preprossessert og tilpasset versjon av KVP10k-datasettet i Hugging Face-format, til å utføre **Key-Value Pair Extraction (KVP)** på  dokumentbilder.\n",
    "\n",
    "- Datasettet består av over 10k forretningsdokumenter, og inneholder blant annet dokumentbilder og tilhørende nøkkel-verdi-par, som brukes av denne fine-tuned modellen som utvikles her.\n",
    "\n",
    "- Sluttmålet er å utvikle og trene en ny modell til dokumentforståelse, ved å forstå **visuell layout**, **tekstlig innhold**, og **relasjoner mellom nøkler og verdier** i dokumentene.\n",
    "  - KVP-Extraction modellen som utvikles i denne notebooken er tenkt å brukes grunnmur i sluttmodellen, for å med stor sannsynlighet beherske å linke mellom nøkkel-verdi-par i ulike dokumenter.\n",
    "\n",
    "LayoutLMv3 er en multi-modal modell designet for å kombinere tekst, layout og annen bilde-informasjon\n",
    "\n",
    "---\n",
    "\n",
    "###**Notbooken dekker følgende steg**:\n",
    "\n",
    "1. Installasjon av de nødendige biblioteker\n",
    "2. Lasting av forhåndsprosessert datasett\n",
    "3. Tokenisering av tekst og input-formatering med Layout sin Processor\n",
    "- 3.1 Logikk for å anngi predikerte BIO-labels til dokumentets bbox'es\n",
    "4. Trening av modell for token-klassifisering\n",
    "5. Evaluering og lagring av modell i Drive\n",
    "6. Visualisering av modell under inferense\n",
    "\n",
    "---\n",
    "\n",
    "###**LayoutLMv3Processor - gjør følgende**:\n",
    "1. Tekst-tokenisering: Tekst fra dokumentet tokeniseres.\n",
    "2. Token-connection: Hvert token kobles til en bounding box (bbox) på dokumentet, gjennom *boxes*-parmeteret som inneholder (x0,y0,x1,y1)-kordinater til hvert token.\n",
    "3. Image-embedding: Dokumentbildet skaleres og legges og blir input til modellen\n",
    "4. Label-alignment: Hvert token får en BIO-label, som brukes under modellens token-klassifisering\n",
    "\n",
    "Tokeniseringen handler om å forvandle dokumentet til tokens med alle nødvendige modaliteter (tekst, layout og bilde) slik at modellen lærer sammenhengen mellom dem gjennom trening.\n",
    "\n",
    "**Det brukes BIO-tagger, og dette er hva det står for:**\n",
    " - B --> Begin: første token i en entitet.\n",
    " - I --> Inside: inne i en entitet.\n",
    " - O --> Outside: tokenen er ikke en del av noen entitet\n",
    "\n",
    "f.eks.\n",
    "  - Tokens:  [\"Name\", \"of\", \"buyer\", \":\", \"Ole\", \"Martin\", \"Lystadmoen\"]\n",
    "  - Labels:  [\"B-KEY\", \"I-KEY\", \"I-KEY\", \"O\", \"B-VALUE\", \"I-VALUE\", \"I-VALUE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset - forståelse\n",
    "\n",
    "**Innhold i train/-mappen i KVP10k:**\n",
    "_____\n",
    "  - *images*/ --> .png bilder av hvert dokument. Visuell input for modellen.\n",
    "    - Det modellen \"ser\".\n",
    "_____\n",
    "\n",
    "  - *ocrs*/ --> JSON-filer med **words** og **bboxes** for hvert dokument. Gir tekst og posisjoner fra OCR, og brukes sammen med images.\n",
    "    - Det modellen \"leser\" (tokens og posisjonene deres).\n",
    "\n",
    "_____\n",
    "\n",
    "  - *gts*/ --> JSON-filer med KVPs og tilhørende bboxes. Inneholder hvilke keys og values som hører sammen.\n",
    "    - Det som lærer modellen hvilke tokens som er nøkler, verdier, og hvilket som er koblet sammen.\n",
    "_____\n",
    "\n",
    "  - *items*/ --> JSON-filer med annotasjoner og layout-objs (rektangler, linker, etiketter)\n",
    "    - tilleggsinformasjon\n",
    "    - ikke viktig i for EE\n",
    "    - Helt nødvendig i RE-delen av dette prosjektet\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformers: Hugging Face bibliotek som gir tilgang til LayoutLMv3\n",
    "#datasets: For håndtering av dataset i Huggig Face-format\n",
    "#seqeval: evalueringsbibliotek for sekvensmerking, brukes for måle metrikker for i dette tilfelle BIO-tagging\n",
    "!pip install -q -U transformers datasets seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Håndterer ulike metrikker inkl. integrasjon med seqeval\n",
    "!pip install -q evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os              #navigere mapper og filer, hente filbaner\n",
    "from PIL import Image  #åpne, vise og manipulere bilder\n",
    "import json            #lese/skrive til JSON-filer\n",
    "from transformers import LayoutLMv3Processor\n",
    "import torch           #modellens input-format for data\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/content/drive/MyDrive/KVP10k-dataset/kvp10k/\"\n",
    "print(os.listdir(base_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False) # <-- Viktig fordi vi allerede har utført OCR på bildet og har tekst og bboxes\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping fra tekstlige BIO-labels til tall som modellen bruker\n",
    "label_map = {\n",
    "    \"O\": 0,\n",
    "    \"B-KEY\": 1,\n",
    "    \"I-KEY\": 2,\n",
    "    \"B-VALUE\": 3,\n",
    "    \"I-VALUE\": 4,\n",
    "}\n",
    "\n",
    "# Funksjon for å skalere bounding boxes til 0-1000 (som LayoutLMv3 krever)\n",
    "def normalize_bbox(bbox, width, height):\n",
    "  return [\n",
    "      int(1000 * (bbox[0] /width)),\n",
    "      int(1000 * (bbox[1] / height)),\n",
    "      int(1000 * (bbox[2] / width)),\n",
    "      int(1000 * (bbox[3] / height))\n",
    "  ]\n",
    "\n",
    "\n",
    "def assign_label_for_box(box, boxes, label_type):\n",
    "  \"\"\"Returnerer liste med (index, label) for tokens som overlapper box\"\"\"\n",
    "  overlaps = []\n",
    "  for i, token_box in enumerate(boxes):\n",
    "    if box_overlap(box, token_box) > 0:\n",
    "      overlaps.append(i)\n",
    "\n",
    "  overlaps = sorted(overlaps)\n",
    "\n",
    "  labeled = []\n",
    "  for j, idx in enumerate(overlaps):\n",
    "    tag = f\"B-{label_type}\" if j == 0 else f\"I-{label_type}\"\n",
    "    labeled.append((idx, tag))\n",
    "\n",
    "  return labeled\n",
    "\n",
    "\n",
    "#Sjekker om OCR-boksen overlapper med GTS(key/value)-boksen.\n",
    "#Ved overlapp hører de til hverandre.\n",
    "def box_overlap(box1, box2):\n",
    "  x0 = max(box1[0], box2[0])\n",
    "  y0 = max(box1[1], box2[1])\n",
    "  x1 = min(box1[2], box2[2])\n",
    "  y1 = min(box1[3], box2[3])\n",
    "  return max(0, x1 - x0) * max(0, y1 - y0)\n",
    "\n",
    "\n",
    "# Funksjon for å generere BIO-labels fra gts (ground truth).\n",
    "# Lager en BIO-label for hvert token basert på om det overlapper med en key- eller value-boks fra GTS.\n",
    "# Matcher hvert token fra OCR (word + bbox) mot key/value-bbokser fra gts:\n",
    "# --> Token overlapper en nøkkelboks: B-KEY eller I-KEY\n",
    "# --> Token overlapper en verdiboks: B-VALUE eller I-VALUE\n",
    "# --> Ellers: O\n",
    "def iob_from_kvps(words, boxes, kvps):\n",
    "  labels = [\"O\"] * len(words)\n",
    "\n",
    "  #Gå igjennom alle key-value-pairs\n",
    "  for kvp in kvps:\n",
    "    if \"key\" in kvp and \"bbox\" in kvp[\"key\"]:\n",
    "      key_bbox = kvp[\"key\"][\"bbox\"]\n",
    "      for idx, tag in assign_label_for_box(key_bbox, boxes, \"KEY\"):\n",
    "        labels[idx] = tag\n",
    "\n",
    "    if \"value\" in kvp and \"bbox\" in kvp[\"value\"]:\n",
    "      value_box = kvp[\"value\"][\"bbox\"]\n",
    "      for idx, tag in assign_label_for_box(value_box, boxes, \"VALUE\"):\n",
    "        labels[idx] = tag\n",
    "\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /content/drive/MyDrive/KVP10k_processed_ready\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Innlasting av et allerede pre-prossesert KVP10k-dataset spesielt utviklet for LayoutLMv3 (KVP-extraction).\n",
    "\n",
    "\n",
    "Henter allerede preprosserert dataset for layout modellen og legger til entiteter og labels for relasjonslaget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Monter Drive (hvis du ikke har gjort det)\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "# Last inn dataset fra riktig path\n",
    "dataset = load_from_disk(\"/content/drive/MyDrive/KVP10k_processed_ready/dataset_all_gts\")\n",
    "\n",
    "# Hent splits\n",
    "train_dataset = dataset[\"train\"].select(range(5000))\n",
    "eval_dataset = dataset[\"eval\"].select(range(1000))\n",
    "test_dataset = dataset[\"test\"].select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def add_entity_pairs_and_labels(example, neg_per_pos: int = 3):\n",
    "    # Hent ut token-labels\n",
    "    labels   = example[\"labels\"]\n",
    "    id2label = {0: \"O\", 1: \"B-KEY\", 2: \"I-KEY\", 3: \"B-VALUE\", 4: \"I-VALUE\"}\n",
    "\n",
    "    # Finn KEY- og VALUE-spans\n",
    "    key_spans, value_spans = [], []\n",
    "    cur_span, cur_type = [], None\n",
    "    for i, lab in enumerate(labels):\n",
    "        if lab == -100:\n",
    "            continue\n",
    "        lbl = id2label[int(lab)]\n",
    "        if lbl.startswith(\"B-\"):\n",
    "            # lagre forrige span\n",
    "            if cur_span:\n",
    "                (key_spans if cur_type==\"KEY\" else value_spans).append(cur_span)\n",
    "            cur_span = [i]\n",
    "            cur_type = lbl.split(\"-\",1)[1]\n",
    "        elif lbl.startswith(\"I-\") and cur_type and lbl.endswith(cur_type):\n",
    "            cur_span.append(i)\n",
    "        else:\n",
    "            if cur_span:\n",
    "                (key_spans if cur_type==\"KEY\" else value_spans).append(cur_span)\n",
    "            cur_span, cur_type = [], None\n",
    "    if cur_span:\n",
    "        (key_spans if cur_type==\"KEY\" else value_spans).append(cur_span)\n",
    "\n",
    "    # Lag positive par\n",
    "    pos_pairs  = [(k, v) for k, v in zip(key_spans, value_spans)]\n",
    "    pos_labels = [1.0] * len(pos_pairs)\n",
    "\n",
    "    # Lag negative par\n",
    "    neg_pairs, neg_labels = [], []\n",
    "    if key_spans and value_spans:\n",
    "        for k in key_spans:\n",
    "            # kandidater som ikke er ekte par\n",
    "            cands = [v for v in value_spans if (k,v) not in pos_pairs]\n",
    "            for v in random.sample(cands, min(len(cands), neg_per_pos)):\n",
    "                neg_pairs.append((k, v))\n",
    "                neg_labels.append(0.0)\n",
    "\n",
    "    # Fallback om ingen par\n",
    "    if not pos_pairs:\n",
    "        pos_pairs, pos_labels = [([0],[1])], [0.0]\n",
    "\n",
    "    # Kombiner og tilordne\n",
    "    example[\"entity_pairs\"] = pos_pairs + neg_pairs\n",
    "    example[\"rel_labels\"]    = pos_labels + neg_labels\n",
    "    return example\n",
    "\n",
    "# Apply to your datasets:\n",
    "train_dataset = train_dataset.map(lambda ex: add_entity_pairs_and_labels(ex, neg_per_pos=3))\n",
    "eval_dataset  = eval_dataset.map(lambda ex: add_entity_pairs_and_labels(ex, neg_per_pos=1))\n",
    "test_dataset  = test_dataset.map(lambda ex: add_entity_pairs_and_labels(ex, neg_per_pos=1))\n",
    "\n",
    "# And then:\n",
    "train_dataset.set_format(\"torch\")\n",
    "eval_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "\n",
    "# Sjekk at alt fungerer\n",
    "print(\"Train:\", len(train_dataset))\n",
    "print(\"Eval:\", len(eval_dataset))\n",
    "print(\"Test:\", len(test_dataset))\n",
    "print(\"Keys:\", train_dataset[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_dataset[0]\n",
    "for k,v in example.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.decode(train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, label in zip(train_dataset[0][\"input_ids\"], train_dataset[0][\"labels\"]):\n",
    "  print(processor.tokenizer.decode([id]), label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "metric = load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-KEY\", \"I-KEY\", \"B-VALUE\", \"I-VALUE\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "\n",
    "    # === Token classification metrics (BIO tagging) – unchanged\n",
    "    token_preds = predictions[\"token_logits\"]\n",
    "    token_labels = labels[\"labels\"]\n",
    "\n",
    "    token_preds = np.argmax(token_preds, axis=2)\n",
    "    true_preds = [\n",
    "        [id2label[p] for (p, l) in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(token_preds, token_labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(token_preds, token_labels)\n",
    "    ]\n",
    "\n",
    "    bio_result = metric.compute(predictions=true_preds, references=true_labels)\n",
    "\n",
    "    # === Relation classification metrics (per‐pair binary) ===\n",
    "    rel_logits = predictions[\"rel_logits\"]    # shape (batch_size, max_pairs)\n",
    "    rel_labels = labels[\"rel_labels\"]         # same shape\n",
    "\n",
    "    if rel_logits is not None and rel_labels is not None:\n",
    "        # sigmoid → threshold → flatten\n",
    "        rel_probs      = 1 / (1 + np.exp(-rel_logits))\n",
    "        rel_preds_bin  = (rel_probs >= 0.5).astype(int).ravel()\n",
    "        rel_labels_flat= np.array(rel_labels, dtype=int).ravel()\n",
    "\n",
    "        rel_acc       = accuracy_score(rel_labels_flat, rel_preds_bin)\n",
    "        rel_f1        = f1_score(rel_labels_flat, rel_preds_bin, average=\"macro\")\n",
    "        rel_precision = precision_score(rel_labels_flat, rel_preds_bin, average=\"macro\")\n",
    "        rel_recall    = recall_score(rel_labels_flat, rel_preds_bin, average=\"macro\")\n",
    "    else:\n",
    "        rel_acc = rel_f1 = rel_precision = rel_recall = 0.0\n",
    "\n",
    "    return {\n",
    "        # BIO tagging\n",
    "        \"precision\": bio_result[\"overall_precision\"],\n",
    "        \"recall\":    bio_result[\"overall_recall\"],\n",
    "        \"f1\":        bio_result[\"overall_f1\"],\n",
    "        \"accuracy\":  bio_result[\"overall_accuracy\"],\n",
    "\n",
    "        # Relation prediction\n",
    "        \"rel_acc\":       rel_acc,\n",
    "        \"rel_f1\":        rel_f1,\n",
    "        \"rel_precision\": rel_precision,\n",
    "        \"rel_recall\":    rel_recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Innlasting av modell, valg av hyperparams og modell-argumenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import LayoutLMv3Model\n",
    "\n",
    "\n",
    "class RelationExtractionHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_relations):\n",
    "        super().__init__()\n",
    "        self.head_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tail_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bilinear = nn.Bilinear(hidden_size, hidden_size, num_relations)\n",
    "\n",
    "    def forward(self, h_i, h_j):\n",
    "        head = self.head_proj(h_i)\n",
    "        tail = self.tail_proj(h_j)\n",
    "        logits = self.bilinear(head, tail)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class LayoutWithRelationModel(nn.Module):\n",
    "    def __init__(self, model_name=\"microsoft/layoutlmv3-base\", hidden_size=768, num_labels=5):\n",
    "        super().__init__()\n",
    "        self.layout = LayoutLMv3Model.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.token_classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.rel_fc = nn.Sequential(\n",
    "            nn.Linear(4 * hidden_size + 1, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def relation(self, h_i, h_j):\n",
    "        h_mul = h_i * h_j\n",
    "        h_diff = h_i - h_j\n",
    "        h_dot = torch.sum(h_i * h_j, dim=-1, keepdim=True)\n",
    "        feats = torch.cat([h_i, h_j, h_mul, h_diff, h_dot], dim=-1)\n",
    "        return self.rel_fc(feats)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        bbox,\n",
    "        image=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        entity_pairs=None,\n",
    "        rel_labels=None\n",
    "    ):\n",
    "        # === Encode with LayoutLMv3\n",
    "        out = self.layout(\n",
    "            input_ids=input_ids,\n",
    "            bbox=bbox,\n",
    "            pixel_values=image,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden_states = self.dropout(out.last_hidden_state)\n",
    "\n",
    "        # === Token classification\n",
    "        token_logits = self.token_classifier(hidden_states)\n",
    "        token_loss = None\n",
    "        token_preds = token_logits.argmax(-1)\n",
    "\n",
    "        if labels is not None:\n",
    "            token_loss = nn.CrossEntropyLoss()(\n",
    "                token_logits.view(-1, token_logits.size(-1)),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "\n",
    "        # === Relation classification\n",
    "        rel_logits = None\n",
    "        rel_loss = None\n",
    "        rel_preds = None\n",
    "\n",
    "        if entity_pairs is not None:\n",
    "            batch_scores = []\n",
    "            for b_idx, pairs in enumerate(entity_pairs):\n",
    "                h = hidden_states[b_idx]\n",
    "                scores = []\n",
    "                for hi_idxs, ti_idxs in pairs:\n",
    "                    h_i = h[hi_idxs].mean(dim=0)\n",
    "                    h_j = h[ti_idxs].mean(dim=0)\n",
    "                    h_mul = h_i * h_j\n",
    "                    h_diff = h_i - h_j\n",
    "                    h_dot = torch.sum(h_i * h_j).unsqueeze(0)\n",
    "                    feats = torch.cat([h_i, h_j, h_mul, h_diff, h_dot], dim=0)\n",
    "                    scores.append(self.rel_fc(feats).squeeze())\n",
    "                batch_scores.append(torch.stack(scores))\n",
    "\n",
    "            # Pad to uniform shape\n",
    "            max_len = max(s.size(0) for s in batch_scores)\n",
    "            padded_logits = [\n",
    "                torch.cat([s, s.new_zeros(max_len - s.size(0))]) if s.size(0) < max_len else s\n",
    "                for s in batch_scores\n",
    "            ]\n",
    "            rel_logits = torch.stack(padded_logits)\n",
    "            rel_preds = (torch.sigmoid(rel_logits) >= 0.5).long()\n",
    "\n",
    "            if rel_labels is not None:\n",
    "                if rel_logits.shape != rel_labels.shape:\n",
    "                    raise ValueError(f\"rel_logits shape {rel_logits.shape} != rel_labels shape {rel_labels.shape}\")\n",
    "                rel_loss = nn.BCEWithLogitsLoss()(rel_logits, rel_labels.float())\n",
    "\n",
    "        # === Total loss\n",
    "        loss = None\n",
    "        if token_loss is not None and rel_loss is not None:\n",
    "            loss = token_loss + 0.5 * rel_loss\n",
    "        elif token_loss is not None:\n",
    "            loss = token_loss\n",
    "        elif rel_loss is not None:\n",
    "            loss = rel_loss\n",
    "\n",
    "        # === Return extended statistics\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"token_logits\": token_logits,\n",
    "            \"token_preds\": token_preds,\n",
    "            \"token_labels\": labels,\n",
    "            \"rel_logits\": rel_logits,\n",
    "            \"rel_preds\": rel_preds,\n",
    "            \"rel_labels\": rel_labels,\n",
    "            \"hidden_states\": hidden_states\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    out = {}\n",
    "    # Stack standard tensors\n",
    "    for k in (\"input_ids\", \"bbox\", \"attention_mask\", \"labels\", \"image\"):\n",
    "        if k in batch[0]:\n",
    "            out[k] = torch.stack([b[k] for b in batch])\n",
    "\n",
    "    # Pad and stack rel_labels\n",
    "    max_rel = max(len(b[\"rel_labels\"]) for b in batch)\n",
    "    rels = []\n",
    "    for b in batch:\n",
    "        lab = b[\"rel_labels\"]\n",
    "        lab = lab.tolist() if isinstance(lab, torch.Tensor) else lab\n",
    "        pad = lab + [0.0] * (max_rel - len(lab))\n",
    "        rels.append(torch.tensor(pad, dtype=torch.float))\n",
    "    out[\"rel_labels\"] = torch.stack(rels)\n",
    "\n",
    "    # Collect entity_pairs directly\n",
    "    out[\"entity_pairs\"] = [b[\"entity_pairs\"] for b in batch]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "eval_loader  = DataLoader(eval_dataset,  batch_size=8, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Trainer oppsett\n",
    "Inneholder:\n",
    "  - Modellen (LayoutLMv3ForTokenClassification)\n",
    "  - Args (hyperparametre som: epochs, batch_size, lr, lr_scheduler,    regularisering, eval_steps, metrics)\n",
    "  - Datasetsplit (train, eval)\n",
    "  - Tokenizer (from processor)\n",
    "  - Collator (litt usikker på denne)\n",
    "  - Metrikker for modellen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# === Freeze backbone if token-only training is needed (optional)\n",
    "def freeze_backbone(model):\n",
    "    for param in model.layout.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# === Full training loop\n",
    "\n",
    "def train_full_model(model, train_loader, eval_loader, optimizer, epochs=2, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    model.to(device)\n",
    "    model.device = device  # <--- Ensure the model has a .device attribute\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch} Train\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"bbox\": batch[\"bbox\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device),\n",
    "                \"image\": batch.get(\"image\", None).to(device) if \"image\" in batch and batch[\"image\"] is not None else None,\n",
    "                \"entity_pairs\": batch.get(\"entity_pairs\"),\n",
    "                \"rel_labels\": batch.get(\"rel_labels\").to(device) if batch.get(\"rel_labels\") is not None else None\n",
    "            }\n",
    "            output = model(**inputs)\n",
    "            loss = output[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch} - Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # === Evaluation\n",
    "        model.eval()\n",
    "        all_token_preds, all_token_labels = [], []\n",
    "        all_rel_preds, all_rel_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_loader, desc=f\"Epoch {epoch} Eval\"):\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"bbox\": batch[\"bbox\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                    \"labels\": batch[\"labels\"].to(device),\n",
    "                    \"image\": batch.get(\"image\", None).to(device) if \"image\" in batch and batch[\"image\"] is not None else None,\n",
    "                    \"entity_pairs\": batch.get(\"entity_pairs\"),\n",
    "                    \"rel_labels\": batch.get(\"rel_labels\").to(device) if batch.get(\"rel_labels\") is not None else None\n",
    "                }\n",
    "                output = model(**inputs)\n",
    "\n",
    "                # === Token metrics\n",
    "                logits = output[\"token_logits\"].argmax(-1).cpu()\n",
    "                label_ids = inputs[\"labels\"].cpu()\n",
    "                for p_seq, l_seq in zip(logits, label_ids):\n",
    "                    for p, l in zip(p_seq.tolist(), l_seq.tolist()):\n",
    "                        if l != -100:\n",
    "                            all_token_preds.append(p)\n",
    "                            all_token_labels.append(l)\n",
    "\n",
    "                # === Relation metrics (pairwise binary predictions)\n",
    "                if output[\"rel_logits\"] is not None and inputs[\"rel_labels\"] is not None:\n",
    "                    rel_logits = output[\"rel_logits\"]\n",
    "                    rel_labels = inputs[\"rel_labels\"]\n",
    "\n",
    "                    for pred_vec, label_vec in zip(torch.sigmoid(rel_logits), rel_labels):\n",
    "                        pred_bin = (pred_vec >= 0.5).long()\n",
    "                        all_rel_preds.extend(pred_bin.cpu().tolist())\n",
    "                        all_rel_labels.extend(label_vec.long().cpu().tolist())\n",
    "\n",
    "        # === Print token classification report\n",
    "        print(\"\\nToken Classification Report:\")\n",
    "        print(classification_report(all_token_labels, all_token_preds, target_names=list(id2label.values())))\n",
    "\n",
    "        # === Print relation classification report (if available)\n",
    "        if all_rel_preds:\n",
    "            print(\"\\nRelation Classification Report:\")\n",
    "            print(classification_report(all_rel_labels, all_rel_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = LayoutWithRelationModel(\n",
    "    model_name=\"microsoft/layoutlmv3-base\",\n",
    "    hidden_size=768,\n",
    "    num_labels=len(id2label)\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "train_full_model(model, train_loader, eval_loader, optimizer, epochs=1, device=device) # TODO endre tilbake til 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = \"/content/drive/MyDrive/KVP10k-layoutlmv3-v6\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Lagre modell\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "processor.save_pretrained(output_dir)\n",
    "\n",
    "# Lagre prosessor\n",
    "processor.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#INFERENCE\n",
    "Laster inn beste fine-tuned modell og dens tilhørende processor fra Drive, samt tilleggsinformasjon som kreves av processoren.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/KVP10k-layoutlmv3-v6\"\n",
    "hidden_size = 768\n",
    "num_labels = len(id2label)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "model = LayoutWithRelationModel(\n",
    "    model_name=\"microsoft/layoutlmv3-base\",\n",
    "    hidden_size=hidden_size,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.load_state_dict(torch.load(f\"{model_path}/pytorch_model.bin\", map_location=\"cpu\"))\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Kode prediksjon og visualisering av dette\n",
    "###*Tokenisering og input-prosessering med Layout sin Processor ved inference*\n",
    "Processor brukes her til å gjøre om tekst, bboxes, og bilde til format modell krever. Dette inkl:\n",
    "- Tokenisering\n",
    "- Normalisering av bboxes tilhørende hvert token\n",
    "- Skalering av bilde\n",
    "- Generering av input-tensorer\n",
    "\n",
    "NB: Denne prosessen gjøres allerede i Data_Processor notebooken som ferdigstilte datasettet for **denne** notebooken. Selve prosessen er dermed nesten indentisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nødvendig for å plassere boksene på originalt-format på bilde-dokumentet\n",
    "def unnormalize_box(bbox, width, height):\n",
    "    return [\n",
    "        width * (bbox[0] / 1000),\n",
    "        height * (bbox[1] / 1000),\n",
    "        width * (bbox[2] / 1000),\n",
    "        height * (bbox[3] / 1000),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "\n",
    "def predict_tokens(doc_id, show_gt=True):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    base_path = \"/content/drive/MyDrive/KVP10k-dataset/kvp10k/test\"\n",
    "    image_path = f\"{base_path}/images/{doc_id}.png\"\n",
    "    ocr_path = f\"{base_path}/ocrs/{doc_id}.json\"\n",
    "    gt_path = f\"{base_path}/gts/{doc_id}.json\"\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    with open(ocr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ocr_data = json.load(f)\n",
    "    with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gt_data = json.load(f)\n",
    "\n",
    "    page = ocr_data[\"pages\"][0]\n",
    "    words = [w[\"text\"] for w in page[\"words\"]]\n",
    "    raw_boxes = [w[\"bbox\"] for w in page[\"words\"]]\n",
    "    width, height = page[\"width\"], page[\"height\"]\n",
    "    norm_boxes = [normalize_bbox(b, width, height) for b in raw_boxes]\n",
    "\n",
    "    string_labels = iob_from_kvps(words, raw_boxes, gt_data[\"kvps_list\"])\n",
    "    word_labels = [label_map.get(lbl, 0) for lbl in string_labels]\n",
    "\n",
    "    encoding = processor(\n",
    "        image,\n",
    "        words,\n",
    "        boxes=norm_boxes,\n",
    "        word_labels=word_labels,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    word_ids = encoding.word_ids()\n",
    "    labels = [-100] * len(word_ids)\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        label_str = string_labels[word_idx]\n",
    "        labels[idx] = label_map[label_str]\n",
    "    encoding[\"labels\"] = torch.tensor([labels])\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in encoding.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            bbox=inputs[\"bbox\"],\n",
    "            image=inputs[\"pixel_values\"],\n",
    "            attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].squeeze().tolist()\n",
    "    predictions = outputs[\"token_logits\"].argmax(-1).squeeze().tolist()\n",
    "    bboxes = inputs[\"bbox\"].squeeze().tolist()\n",
    "    unnorm_boxes = [unnormalize_box(b, width, height) for b in bboxes]\n",
    "    tokens = [processor.tokenizer.decode([tid]) for tid in input_ids]\n",
    "\n",
    "    filtered = [\n",
    "        (token, id2label.get(pred, \"O\"), box)\n",
    "        for token, pred, box in zip(tokens, predictions, unnorm_boxes)\n",
    "        if token not in [\"[PAD]\", \"[CLS]\", \"[SEP]\"]\n",
    "    ]\n",
    "\n",
    "    print(\"Unike labels i output:\", set(p[1] for p in filtered))\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    def iob_to_label(label):\n",
    "        return label[2:].lower() if label.startswith((\"B-\", \"I-\")) else \"other\"\n",
    "\n",
    "    label2color = {\n",
    "        \"key\": \"blue\",\n",
    "        \"value\": \"green\",\n",
    "        \"other\": \"gray\"\n",
    "    }\n",
    "\n",
    "    for token, pred_label, box in filtered:\n",
    "        if pred_label == \"O\":\n",
    "            continue\n",
    "        label_type = iob_to_label(pred_label)\n",
    "        draw.rectangle(box, outline=label2color.get(label_type, \"red\"), width=2)\n",
    "        draw.text((box[0] + 5, box[1] - 10), label_type, fill=label2color.get(label_type, \"red\"), font=font)\n",
    "\n",
    "    print(\"Modellens prediksjoner:\")\n",
    "    display(image)\n",
    "\n",
    "    if show_gt:\n",
    "        gt_img = Image.open(image_path).convert(\"RGB\")\n",
    "        draw_gt = ImageDraw.Draw(gt_img)\n",
    "        for word, box, label_id in zip(words, raw_boxes, string_labels):\n",
    "            if label_id == \"O\":\n",
    "                continue\n",
    "            label_type = iob_to_label(label_id)\n",
    "            draw_gt.rectangle(box, outline=label2color.get(label_type, \"gray\"), width=2)\n",
    "            draw_gt.text((box[0] + 5, box[1] - 10), label_type, fill=label2color.get(label_type, \"gray\"), font=font)\n",
    "\n",
    "        print(\"Ground Truth:\")\n",
    "        display(gt_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Velg vilkårlig dokument fra datasettet og prediker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_token_head(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            bbox = batch[\"bbox\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                bbox=bbox,\n",
    "                attention_mask=attention_mask,\n",
    "                image=batch.get(\"image\")\n",
    "            )\n",
    "            token_logits = output[\"token_logits\"]\n",
    "            preds = token_logits.argmax(-1)\n",
    "\n",
    "            for p_seq, l_seq in zip(preds, labels):\n",
    "                for p, l in zip(p_seq.cpu().tolist(), l_seq.cpu().tolist()):\n",
    "                    if l != -100:\n",
    "                        all_preds.append(p)\n",
    "                        all_labels.append(l)\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, target_names=list(id2label.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Velg dokument\n",
    "#document_selected = \"aaf643426f0250efd10de3d9df63b407292f3fcc2aa335e399c37aca32443ea1\"\n",
    "document_selected = \"aaed61e79aa3edbae844f5775789ebb6aa1a94a23d9cb3468d2cfc974af304e5\"\n",
    "# document_selected = \"aa35720ba3611f946c372cc99d8cd1d78e81265b8ceb51dcdb4672d196944c2b\"\n",
    "# document_selected = \"faa5d71172e2e9959b41a5aec4fd2ab700534d1b2729484d2d5f26472cd56cfa\"\n",
    "# document_selected = \"ffe462e43b9dff12e78ea8fb69332abfb789da171a8597f5bb961853e06e6fa2\"\n",
    "# document_selected = \"feb2c4b21388318c7a51cc0aaf0e7c673a07f5204a40549a281bef065bb77925\"\n",
    "# document_selected = \"feaf84d435bd46100db82de51f5a989ff4d39fdcdb040a7044720b943e34b7d7\"\n",
    "# document_selected = \"df6b0a4cf1908bb95be874e4efa59411c685095d7bb596879961563503b5c239\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_token_head(model, eval_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tokens(document_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "import json, torch\n",
    "\n",
    "def predict_relations(doc_id: str, threshold: float = 0.5):\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    import json, torch\n",
    "\n",
    "    # — same helper as before —\n",
    "    def extract_spans(labels, kind):\n",
    "        spans, cur = [], []\n",
    "        for idx, lab in enumerate(labels):\n",
    "            if lab == f\"B-{kind}\":\n",
    "                if cur: spans.append(cur)\n",
    "                cur = [idx]\n",
    "            elif lab == f\"I-{kind}\" and cur:\n",
    "                cur.append(idx)\n",
    "            else:\n",
    "                if cur: spans.append(cur)\n",
    "                cur = []\n",
    "        if cur: spans.append(cur)\n",
    "        return spans\n",
    "\n",
    "    # — load image & OCR JSON —\n",
    "    base     = \"/content/drive/MyDrive/KVP10k-dataset/kvp10k/test\"\n",
    "    img_path = f\"{base}/images/{doc_id}.png\"\n",
    "    ocr_path = f\"{base}/ocrs/{doc_id}.json\"\n",
    "    image    = Image.open(img_path).convert(\"RGB\")\n",
    "    with open(ocr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ocr = json.load(f)\n",
    "\n",
    "    page       = ocr[\"pages\"][0]\n",
    "    words      = [w[\"text\"] for w in page[\"words\"]]\n",
    "    raw_boxes  = [w[\"bbox\"] for w in page[\"words\"]]\n",
    "    W, H       = page[\"width\"], page[\"height\"]\n",
    "    norm_boxes = [normalize_bbox(b, W, H) for b in raw_boxes]\n",
    "\n",
    "    # — run processor & model —\n",
    "    enc = processor(image, words, boxes=norm_boxes,\n",
    "                    return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
    "    token_boxes = enc[\"bbox\"][0].cpu().tolist()\n",
    "    for k in (\"input_ids\",\"bbox\",\"pixel_values\",\"attention_mask\"):\n",
    "        enc[k] = enc[k].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=enc[\"input_ids\"],\n",
    "                    bbox=enc[\"bbox\"],\n",
    "                    image=enc[\"pixel_values\"],\n",
    "                    attention_mask=enc[\"attention_mask\"])\n",
    "\n",
    "    pred_ids    = out[\"token_logits\"][0].argmax(-1).tolist()\n",
    "    labels_pred = [id2label.get(i, \"O\") for i in pred_ids]\n",
    "    hiddens     = out[\"hidden_states\"][0]\n",
    "\n",
    "    # — extract spans —\n",
    "    key_spans   = extract_spans(labels_pred, \"KEY\")\n",
    "    value_spans = extract_spans(labels_pred, \"VALUE\")\n",
    "\n",
    "    # — score & collect all relations above threshold —\n",
    "    relations = []\n",
    "    for k_span in key_spans:\n",
    "        for v_span in value_spans:\n",
    "            h_k   = hiddens[k_span].mean(dim=0, keepdim=True)\n",
    "            h_v   = hiddens[v_span].mean(dim=0, keepdim=True)\n",
    "            prob  = torch.sigmoid(model.relation(h_k, h_v)).item()\n",
    "            if prob >= threshold:\n",
    "                relations.append((k_span, v_span, prob))\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    if not relations:\n",
    "        draw.text((10,10), \"Ingen relasjoner funnet\", fill=\"orange\", font=font)\n",
    "    else:\n",
    "        # ← pick the single most likely pair\n",
    "        k_span, v_span, prob = max(relations, key=lambda x: x[2])\n",
    "\n",
    "        # draw that one\n",
    "        for i in k_span:\n",
    "            draw.rectangle(unnormalize_box(token_boxes[i], W, H),\n",
    "                           outline=\"blue\", width=2)\n",
    "        for i in v_span:\n",
    "            draw.rectangle(unnormalize_box(token_boxes[i], W, H),\n",
    "                           outline=\"green\", width=2)\n",
    "\n",
    "        # recompute its centers\n",
    "        kxs = [(b[0]+b[2])/2 for idx in k_span for b in [token_boxes[idx]]]\n",
    "        kys = [(b[1]+b[3])/2 for idx in k_span for b in [token_boxes[idx]]]\n",
    "        vxs = [(b[0]+b[2])/2 for idx in v_span for b in [token_boxes[idx]]]\n",
    "        vys = [(b[1]+b[3])/2 for idx in v_span for b in [token_boxes[idx]]]\n",
    "\n",
    "        kc = ((sum(kxs)/len(kxs))*W/1000, (sum(kys)/len(kys))*H/1000)\n",
    "        vc = ((sum(vxs)/len(vxs))*W/1000, (sum(vys)/len(vys))*H/1000)\n",
    "\n",
    "        draw.line([kc, vc], fill=\"red\", width=2)\n",
    "        mx, my = (kc[0]+vc[0])/2, (kc[1]+vc[1])/2\n",
    "        draw.text((mx, my), f\"{prob:.2f}\", fill=\"red\", font=font)\n",
    "\n",
    "    display(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "import json, torch\n",
    "import math\n",
    "\n",
    "def predict_relations_per_key(doc_id: str, threshold: float = 0.5):\n",
    "    # ─── helper to extract BIO spans ──────────────────────────────\n",
    "    def extract_spans(labels, kind):\n",
    "        spans, cur = [], []\n",
    "        for idx, lab in enumerate(labels):\n",
    "            if lab == f\"B-{kind}\":\n",
    "                if cur: spans.append(cur)\n",
    "                cur = [idx]\n",
    "            elif lab == f\"I-{kind}\" and cur:\n",
    "                cur.append(idx)\n",
    "            else:\n",
    "                if cur: spans.append(cur)\n",
    "                cur = []\n",
    "        if cur: spans.append(cur)\n",
    "        return spans\n",
    "\n",
    "    # ─── load image & OCR ─────────────────────────────────────────\n",
    "    base     = \"/content/drive/MyDrive/KVP10k-dataset/kvp10k/test\"\n",
    "    img_path = f\"{base}/images/{doc_id}.png\"\n",
    "    ocr_path = f\"{base}/ocrs/{doc_id}.json\"\n",
    "    image    = Image.open(img_path).convert(\"RGB\")\n",
    "    with open(ocr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ocr = json.load(f)\n",
    "\n",
    "    page      = ocr[\"pages\"][0]\n",
    "    words     = [w[\"text\"] for w in page[\"words\"]]\n",
    "    raw_boxes = [w[\"bbox\"] for w in page[\"words\"]]\n",
    "    W, H      = page[\"width\"], page[\"height\"]\n",
    "    norm_boxes= [normalize_bbox(b, W, H) for b in raw_boxes]\n",
    "\n",
    "    # ─── run processor & model ────────────────────────────────────\n",
    "    enc = processor(image, words, boxes=norm_boxes,\n",
    "                    return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
    "    token_boxes = enc[\"bbox\"][0].cpu().tolist()\n",
    "    for k in (\"input_ids\",\"bbox\",\"pixel_values\",\"attention_mask\"):\n",
    "        enc[k] = enc[k].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids      = enc[\"input_ids\"],\n",
    "            bbox           = enc[\"bbox\"],\n",
    "            image          = enc[\"pixel_values\"],\n",
    "            attention_mask = enc[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "    # ─── decode predictions & get hidden states ──────────────────\n",
    "    pred_ids    = out[\"token_logits\"][0].argmax(-1).tolist()\n",
    "    labels_pred = [id2label.get(i, \"O\") for i in pred_ids]\n",
    "    hiddens     = out[\"hidden_states\"][0]  # (seq_len, hidden)\n",
    "\n",
    "    # ─── find key/value spans ─────────────────────────────────────\n",
    "    key_spans   = extract_spans(labels_pred, \"KEY\")\n",
    "    value_spans = extract_spans(labels_pred, \"VALUE\")\n",
    "\n",
    "    # ─── for each key, pick its best value ────────────────────────\n",
    "    relations = []\n",
    "    for k_span in key_spans:\n",
    "        best_prob = 0.0\n",
    "        best_v    = None\n",
    "        for v_span in value_spans:\n",
    "            h_k  = hiddens[k_span].mean(dim=0, keepdim=True)\n",
    "            h_v  = hiddens[v_span].mean(dim=0, keepdim=True)\n",
    "            prob = torch.sigmoid(model.relation(h_k, h_v)).item()\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_v    = v_span\n",
    "        if best_v is not None and best_prob >= threshold:\n",
    "            relations.append((k_span, best_v, best_prob))\n",
    "\n",
    "    # ─── draw all top‐links with arrows ───────────────────────────\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "    arrow_len = 15    # pixels\n",
    "    arrow_angle = math.radians(25)\n",
    "\n",
    "    if not relations:\n",
    "        draw.text((10,10), \"Ingen relasjoner funnet\", fill=\"orange\", font=font)\n",
    "    else:\n",
    "        for k_span, v_span, prob in relations:\n",
    "            # box outlines\n",
    "            for i in k_span:\n",
    "                draw.rectangle(unnormalize_box(token_boxes[i], W, H),\n",
    "                               outline=\"blue\", width=2)\n",
    "            for i in v_span:\n",
    "                draw.rectangle(unnormalize_box(token_boxes[i], W, H),\n",
    "                               outline=\"green\", width=2)\n",
    "\n",
    "            # compute pixel centers\n",
    "            kxs = [(b[0]+b[2])/2 for idx in k_span for b in [token_boxes[idx]]]\n",
    "            kys = [(b[1]+b[3])/2 for idx in k_span for b in [token_boxes[idx]]]\n",
    "            vxs = [(b[0]+b[2])/2 for idx in v_span for b in [token_boxes[idx]]]\n",
    "            vys = [(b[1]+b[3])/2 for idx in v_span for b in [token_boxes[idx]]]\n",
    "            kc = ((sum(kxs)/len(kxs))*W/1000, (sum(kys)/len(kys))*H/1000)\n",
    "            vc = ((sum(vxs)/len(vxs))*W/1000, (sum(vys)/len(vys))*H/1000)\n",
    "\n",
    "            # draw main shaft of arrow (shortened so arrowhead fits)\n",
    "            dx, dy = vc[0]-kc[0], vc[1]-kc[1]\n",
    "            dist = math.hypot(dx, dy)\n",
    "            if dist > arrow_len:\n",
    "                ux, uy = dx/dist, dy/dist\n",
    "                tail = (kc[0], kc[1])\n",
    "                head_base = (vc[0] - ux*arrow_len, vc[1] - uy*arrow_len)\n",
    "                draw.line([tail, head_base], fill=\"red\", width=2)\n",
    "            else:\n",
    "                head_base = kc  # if very short, draw full\n",
    "\n",
    "            # compute arrowhead points\n",
    "            ux, uy = (vc[0]-head_base[0])/arrow_len, (vc[1]-head_base[1])/arrow_len\n",
    "            # two wing directions\n",
    "            left_x  = vc[0] - arrow_len*(ux*math.cos(arrow_angle) + uy*math.sin(arrow_angle))\n",
    "            left_y  = vc[1] - arrow_len*(uy*math.cos(arrow_angle) - ux*math.sin(arrow_angle))\n",
    "            right_x = vc[0] - arrow_len*(ux*math.cos(arrow_angle) - uy*math.sin(arrow_angle))\n",
    "            right_y = vc[1] - arrow_len*(uy*math.cos(arrow_angle) + ux*math.sin(arrow_angle))\n",
    "            draw.polygon([vc, (left_x,left_y), (right_x,right_y)], fill=\"red\")\n",
    "\n",
    "            # draw probability label just above arrowhead\n",
    "            draw.text((vc[0]+3, vc[1]-10), f\"{prob:.2f}\",\n",
    "                      fill=\"red\", font=font)\n",
    "\n",
    "    display(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_relations_per_key(document_selected)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04855b0ccf5d4fd7a15fab191dd96fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d295300697048698954e61e72aa9893",
      "placeholder": "​",
      "style": "IPY_MODEL_9e70f0dd27db40ef8a47ebd2302091b4",
      "value": "Epoch 1 Eval: 100%"
     }
    },
    "07ac32bde8254462b599396d43491875": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "137d6c8435c94840900b076939dc657c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1990a2b6f4d143f1b764ba7983c4f1e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_04855b0ccf5d4fd7a15fab191dd96fa0",
       "IPY_MODEL_23aa0c25c404480b89e729b65e1465a3",
       "IPY_MODEL_f562972b55184e678c00c27381d1d269"
      ],
      "layout": "IPY_MODEL_6ada4bfa10b5411c83cf1857220137a0"
     }
    },
    "1c6cdfce4259496ca4b40ef0bfe7c82b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_495317d1fffa4a18be3f97afaa521d14",
      "max": 625,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_137d6c8435c94840900b076939dc657c",
      "value": 625
     }
    },
    "23aa0c25c404480b89e729b65e1465a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e79b5400ff1f41509bb9dc7757bfc6bc",
      "max": 125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_07ac32bde8254462b599396d43491875",
      "value": 125
     }
    },
    "28b76c3785bd444c8518b9d4c92ce711": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b239812c0314c4d94a6cf735f87063d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d38d9ca139b4783bb8c2b35a999e8ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "495317d1fffa4a18be3f97afaa521d14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4be28d16155a49d78961ac2467291b9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d38d9ca139b4783bb8c2b35a999e8ef",
      "placeholder": "​",
      "style": "IPY_MODEL_2b239812c0314c4d94a6cf735f87063d",
      "value": "Epoch 1 Train: 100%"
     }
    },
    "5006311d93104e05a4af7640b5a970e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ada4bfa10b5411c83cf1857220137a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d295300697048698954e61e72aa9893": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82ed922846f74b3b87be195d95bf12f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90cba68eef1446ada0280c2f7ba06008",
      "placeholder": "​",
      "style": "IPY_MODEL_ea4b7b96f2a140778b62de3b07b73073",
      "value": " 625/625 [11:52&lt;00:00,  1.20s/it]"
     }
    },
    "90cba68eef1446ada0280c2f7ba06008": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e70f0dd27db40ef8a47ebd2302091b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b74678c109c14ec5822868345ad7aed0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c71ad1abaa604a5697af3eaf7a40c9f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4be28d16155a49d78961ac2467291b9d",
       "IPY_MODEL_1c6cdfce4259496ca4b40ef0bfe7c82b",
       "IPY_MODEL_82ed922846f74b3b87be195d95bf12f0"
      ],
      "layout": "IPY_MODEL_28b76c3785bd444c8518b9d4c92ce711"
     }
    },
    "e79b5400ff1f41509bb9dc7757bfc6bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea4b7b96f2a140778b62de3b07b73073": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f562972b55184e678c00c27381d1d269": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b74678c109c14ec5822868345ad7aed0",
      "placeholder": "​",
      "style": "IPY_MODEL_5006311d93104e05a4af7640b5a970e9",
      "value": " 125/125 [00:58&lt;00:00,  3.41it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
