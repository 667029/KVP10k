{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfR80b+XuSQs0x2ss8hRgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/667029/KVP10k/blob/main/Relation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Setup\n",
        "In this section, we import necessary libraries (e.g., PyTorch, TensorFlow, and Hugging Face `datasets`) and configure any global settings."
      ],
      "metadata": {
        "id": "ELQloE1JFU4v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOl8HNT2EvpM"
      },
      "outputs": [],
      "source": [
        "!pip install datasets torch transformers accelerate numpy tqdm tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definerer drive lokasjon\n",
        "drive_mount_path='/content/drive'\n",
        "\n",
        "hidden_size = 768           # Size of the hidden vectors.\n",
        "batch_size = 32             # Batch size.\n",
        "test_size_percentage = 10   # Percentage of dataset to be dedicated to test\n",
        "seed = 42                   # Seed for reproducibility.\n",
        "\n",
        "\n",
        "# Paths to your datasets\n",
        "train_dataset_path = \"/content/drive/MyDrive/RE_ready/re_dataset_train_combined\"\n",
        "test_dataset_path = \"/content/drive/MyDrive/RE_ready/re_dataset_test_combined\"\n"
      ],
      "metadata": {
        "id": "IX4K8fBMEyQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n",
        "Load the prepared training, evaluation, and test datasets from disk using HF `load_from_disk`."
      ],
      "metadata": {
        "id": "ITVBGvmgFhKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_from_disk\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DataUtil:\n",
        "    @staticmethod\n",
        "    def c(dataset_path=None, model_output=None, drive_mount_path=drive_mount_path, extract_hidden=False, verbose=True):\n",
        "        if dataset_path is not None:\n",
        "            try:\n",
        "                from google.colab import drive\n",
        "                drive.mount(drive_mount_path, force_remount=False)\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(\"Google Drive may already be mounted. Continuing...\")\n",
        "            dataset = load_from_disk(dataset_path)\n",
        "            if verbose:\n",
        "                print(\"Loaded dataset from drive.\")\n",
        "                print(\"Number of samples:\", len(dataset))\n",
        "                print(\"Column names:\", dataset.column_names)\n",
        "                print(\"First sample:\", dataset[0])\n",
        "            return dataset\n",
        "        elif model_output is not None:\n",
        "            if extract_hidden:\n",
        "                if isinstance(model_output, dict) and \"hidden_states\" in model_output:\n",
        "                    model_output = model_output[\"hidden_states\"]\n",
        "                elif isinstance(model_output, (list, tuple)):\n",
        "                    model_output = model_output[-1]\n",
        "            if verbose:\n",
        "                print(\"Provided model output:\")\n",
        "                print(model_output)\n",
        "            return model_output\n",
        "        else:\n",
        "            raise ValueError(\"Please provide either a dataset_path or a model_output.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def to_tf_dataset(dataset, hidden_size, batch_size=32, shuffle_buffer=1000, verbose=True):\n",
        "        def generator():\n",
        "            for sample in dataset:\n",
        "                h_i = np.array(sample[\"h_i\"], dtype=np.float32)\n",
        "                h_j = np.array(sample[\"h_j\"], dtype=np.float32)\n",
        "                label_arr = np.array(sample[\"label\"], dtype=np.int32)\n",
        "                label = label_arr[0] if label_arr.ndim > 0 else label_arr\n",
        "                key = sample[\"key\"] if \"key\" in sample else \"unknown\"\n",
        "                yield (h_i, h_j, label, key)\n",
        "\n",
        "        first_sample = dataset[0]\n",
        "        h_i_first = np.array(first_sample[\"h_i\"], dtype=np.float32)\n",
        "\n",
        "        if h_i_first.ndim == 1:\n",
        "            output_signature = (\n",
        "                tf.TensorSpec(shape=(hidden_size,), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(hidden_size,), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.int32),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.string),\n",
        "            )\n",
        "            ds = tf.data.Dataset.from_generator(generator, output_signature=output_signature)\n",
        "            ds = ds.shuffle(shuffle_buffer).batch(batch_size)\n",
        "        elif h_i_first.ndim == 2:\n",
        "            output_signature = (\n",
        "                tf.TensorSpec(shape=(None, hidden_size), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(None, hidden_size), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.int32),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.string),\n",
        "            )\n",
        "            ds = tf.data.Dataset.from_generator(generator, output_signature=output_signature)\n",
        "            ds = ds.shuffle(shuffle_buffer).padded_batch(\n",
        "                batch_size,\n",
        "                padded_shapes=([None, hidden_size], [None, hidden_size], [], [])\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Expected h_i to be 1D or 2D; got shape \" + str(h_i_first.shape))\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Converted dataset to tf.data.Dataset with batch size {batch_size}.\")\n",
        "        return ds\n",
        "\n"
      ],
      "metadata": {
        "id": "d7yup6goE15B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop\n",
        "Train the model: iterate over epochs and batches, compute loss, and update model weights."
      ],
      "metadata": {
        "id": "cmm7VyalFnQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training dataset\n",
        "train_loaded_dataset = DataUtil.get_output(dataset_path=train_dataset_path)\n",
        "split_dataset = train_loaded_dataset.train_test_split(test_size=test_size_percentage/100, seed=seed)\n",
        "\n",
        "## Hent dataset fra output av layoutmvl3 modellen\n",
        "# train_loaded_dataset = DataUtil.get_output(model_output=outputs, verbose=True)\n",
        "# split_dataset = train_loaded_dataset.train_test_split(test_size=0.2, seed=seed)\n",
        "\n",
        "\n",
        "# Explicitly naming sets\n",
        "train_dataset_hf = split_dataset[\"train\"]\n",
        "eval_dataset_hf = split_dataset[\"test\"]\n",
        "\n",
        "\n",
        "# Load your separate test dataset for inference/evaluation\n",
        "test_loaded_dataset = DataUtil.get_output(dataset_path=test_dataset_path)\n",
        "test_dataset_hf = test_loaded_dataset\n"
      ],
      "metadata": {
        "id": "XWJbwLU2E5_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Oppretter trening og test set\n",
        "train_dataset = DataUtil.to_tf_dataset(train_dataset_hf, hidden_size=hidden_size, batch_size=32)\n",
        "for batch in train_dataset.take(1):\n",
        "    h_i_batch, h_j_batch, labels_batch, keys_batch = batch\n",
        "    print(\"h_i batch shape:\", h_i_batch.shape)\n",
        "    print(\"h_j batch shape:\", h_j_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    print(\"keys batch shape:\", keys_batch.shape)\n",
        "\n",
        "test_dataset = DataUtil.to_tf_dataset(test_dataset_hf, hidden_size=hidden_size, batch_size=32)\n",
        "for batch in test_dataset.take(1):\n",
        "    h_i_batch, h_j_batch, labels_batch, keys_batch = batch\n",
        "    print(\"h_i batch shape:\", h_i_batch.shape)\n",
        "    print(\"h_j batch shape:\", h_j_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    print(\"keys batch shape:\", keys_batch.shape)"
      ],
      "metadata": {
        "id": "S9iUEP5nE-0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class Bilinear(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_size, num_relations):\n",
        "        super(Bilinear, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_relations = num_relations\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create weight tensor of shape (hidden_size, num_relations, hidden_size)\n",
        "        self.W = self.add_weight(\n",
        "            shape=(self.hidden_size, self.num_relations, self.hidden_size),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name=\"bilinear_W\"\n",
        "        )\n",
        "        # Bias vector of shape (num_relations,)\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(self.num_relations,),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            name=\"bilinear_bias\"\n",
        "        )\n",
        "        super(Bilinear, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        head, tail = inputs\n",
        "        # Use einsum to compute: for each sample, head^T * W * tail.\n",
        "        # Equation: 'bi,irk,bj->br' where:\n",
        "        #   b: batch, i: hidden dimension from head, j: hidden dimension from tail, r: relation index.\n",
        "        logits = tf.einsum('bi,irk,bj->br', head, self.W, tail)\n",
        "        logits = logits + self.bias\n",
        "        return logits"
      ],
      "metadata": {
        "id": "BuOpWr4cFA_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition\n",
        "Define the Relation Link layer/model components, including custom layers and heads."
      ],
      "metadata": {
        "id": "yXst-sVpFr8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class RelationExtractionHead(tf.keras.Model):\n",
        "    def __init__(self, hidden_size, num_relations):\n",
        "        super(RelationExtractionHead, self).__init__()\n",
        "        self.head_proj = tf.keras.layers.Dense(hidden_size, name=\"head_proj\")\n",
        "        self.tail_proj = tf.keras.layers.Dense(hidden_size, name=\"tail_proj\")\n",
        "        self.bilinear = Bilinear(hidden_size, num_relations)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        h_i, h_j = inputs\n",
        "        head = self.head_proj(h_i)\n",
        "        tail = self.tail_proj(h_j)\n",
        "        logits = self.bilinear((head, tail))\n",
        "        return logits"
      ],
      "metadata": {
        "id": "717--fBCFDiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Train the model..."
      ],
      "metadata": {
        "id": "anXRgssXQIeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "num_relations = 5\n",
        "num_epochs = 1\n",
        "learning_rate = 1e-4\n",
        "num_train_samples = len(train_dataset_hf)\n",
        "sample_base = int(np.ceil(num_train_samples / num_epochs))\n",
        "sample_range_min = int(sample_base * 0.95)\n",
        "sample_range_max = int(sample_base * 1.10)"
      ],
      "metadata": {
        "id": "CR91_o7EP6u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Model and optimizer\n",
        "model_tf = RelationExtractionHead(hidden_size=hidden_size, num_relations=num_relations)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    samples_this_epoch = random.randint(sample_range_min, sample_range_max)\n",
        "    total_batches = int(np.ceil(samples_this_epoch / batch_size))\n",
        "\n",
        "    epoch_dataset = (\n",
        "        train_dataset.unbatch()\n",
        "        .shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
        "        .take(samples_this_epoch)\n",
        "        .batch(batch_size)\n",
        "    )\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, sampling {samples_this_epoch} examples...\")\n",
        "    epoch_loss = []\n",
        "\n",
        "    for batch, (h_i_batch, h_j_batch, labels_batch, keys_batch) in enumerate(tqdm(epoch_dataset, total=total_batches, desc=f\"Epoch {epoch+1}\", unit=\"batch\")):\n",
        "        if len(h_i_batch.shape) == 3:\n",
        "            h_i_batch = tf.reduce_mean(h_i_batch, axis=1)\n",
        "            h_j_batch = tf.reduce_mean(h_j_batch, axis=1)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model_tf((h_i_batch, h_j_batch), training=True)\n",
        "            loss = loss_fn(labels_batch, logits)\n",
        "\n",
        "        grads = tape.gradient(loss, model_tf.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model_tf.trainable_variables))\n",
        "        epoch_loss.append(loss.numpy())\n",
        "\n",
        "    avg_loss = np.mean(epoch_loss)\n",
        "    loss_history.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, num_epochs + 1), loss_history, marker='o', linestyle='-')\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Us71_c5fFFmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "Run model evaluation on the test dataset, computing predictions, probabilities, and performance metrics."
      ],
      "metadata": {
        "id": "AWZRaVTJFwc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "\n",
        "class RelationModelTesterTF:\n",
        "    def __init__(self, model, dataset, batch_size=32, num_samples=None):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        # estimate num_samples if not provided\n",
        "        self.num_samples = num_samples or sum(1 for _ in dataset)\n",
        "        self.total_batches = int(np.ceil(self.num_samples / batch_size))\n",
        "\n",
        "    def evaluate(self):\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "        all_keys = []\n",
        "        all_h_i = []\n",
        "        all_h_j = []\n",
        "\n",
        "        bar = tqdm(self.dataset, desc=\"Evaluating\", total=self.total_batches, unit=\"batch\")\n",
        "        for h_i, h_j, labels, keys in bar:\n",
        "            # pool sequences if 3D\n",
        "            if len(h_i.shape) == 3:\n",
        "                h_i = tf.reduce_mean(h_i, axis=1)\n",
        "                h_j = tf.reduce_mean(h_j, axis=1)\n",
        "\n",
        "            logits = self.model((h_i, h_j), training=False)\n",
        "            probs  = tf.nn.softmax(logits, axis=-1)\n",
        "            preds  = tf.argmax(probs, axis=-1)\n",
        "\n",
        "            all_preds.extend(preds.numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "            all_probs.extend(probs.numpy())\n",
        "            all_keys.extend(keys.numpy())\n",
        "            all_h_i.extend(h_i.numpy())\n",
        "            all_h_j.extend(h_j.numpy())\n",
        "\n",
        "        # Core metrics\n",
        "        accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            all_labels, all_preds, average=\"weighted\"\n",
        "        )\n",
        "        report_str = classification_report(all_labels, all_preds)\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"predictions\": all_preds,\n",
        "            \"ground_truth\": all_labels,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"classification_report\": report_str,\n",
        "            \"probabilities\": all_probs,\n",
        "            \"keys\": all_keys,\n",
        "            \"h_i\": all_h_i,\n",
        "            \"h_j\": all_h_j,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "59G7qkI6FIcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation\n",
        "tester = RelationModelTesterTF(\n",
        "    model_tf,\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    num_samples=len(test_dataset_hf)\n",
        ")\n",
        "results = tester.evaluate()\n",
        "\n",
        "# Print overall metrics\n",
        "print(f\"Test Accuracy       : {results['accuracy']:.2%}\")\n",
        "print(f\"Weighted Precision  : {results['precision']:.4f}\")\n",
        "print(f\"Weighted Recall     : {results['recall']:.4f}\")\n",
        "print(f\"Weighted F1‑score   : {results['f1']:.4f}\\n\")\n",
        "\n",
        "# Print some example predictions\n",
        "print(\"Example Predictions (first 10 samples):\", results[\"predictions\"][:10])\n",
        "\n",
        "# Detailed sample‑wise breakdown\n",
        "print(\"\\nSample-wise Predictions:\")\n",
        "for i in range(min(10, len(results[\"predictions\"]))):\n",
        "    print(f\"\\nSample {i}\")\n",
        "    print(\"  True Label      :\", results[\"ground_truth\"][i])\n",
        "    print(\"  Predicted Label :\", results[\"predictions\"][i])\n",
        "    print(\"  Probabilities   :\", np.round(results[\"probabilities\"][i], 4))\n",
        "    print(\"  h_i (first 5)   :\", np.round(results[\"h_i\"][i][:5], 4))\n",
        "    print(\"  h_j (first 5)   :\", np.round(results[\"h_j\"][i][:5], 4))\n"
      ],
      "metadata": {
        "id": "muiGnGJnFMJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}