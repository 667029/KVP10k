{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPBA14hxjfhF+T6Zh1j0y3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/667029/KVP10k/blob/main/Dataset_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5iEgTniutQi"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets seqeval\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os              #navigere mapper og filer, hente filbaner\n",
        "from PIL import Image  #åpne, vise og manipulere bilder\n",
        "import json            #lese/skrive til JSON-filer\n",
        "from transformers import LayoutLMv3Processor\n",
        "import torch\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "WZ3b2UXFuzKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "base_path = \"/content/drive/MyDrive/DAT255/KVP10k-dataset/kvp10k/\"\n",
        "print(os.listdir(base_path))"
      ],
      "metadata": {
        "id": "emr5Ma8iuzyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False) # <-- Viktig fordi vi allerede har utført OCR på bildet og har tekst og bboxes"
      ],
      "metadata": {
        "id": "1fkhIhm6u4hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports ===\n",
        "from datasets import Dataset, DatasetDict, Features, Sequence, Value, Array2D, Array3D, Image as HFImage\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "# === Label map for KVP ===\n",
        "label_map = {\n",
        "    \"O\": 0,\n",
        "    \"B-KEY\": 1,\n",
        "    \"I-KEY\": 2,\n",
        "    \"B-VALUE\": 3,\n",
        "    \"I-VALUE\": 4,\n",
        "}\n",
        "\n",
        "# === BBox helpers ===\n",
        "def normalize_bbox(bbox, width, height):\n",
        "    return [\n",
        "        int(1000 * (bbox[0] / width)),\n",
        "        int(1000 * (bbox[1] / height)),\n",
        "        int(1000 * (bbox[2] / width)),\n",
        "        int(1000 * (bbox[3] / height))\n",
        "    ]\n",
        "\n",
        "def box_overlap(box1, box2):\n",
        "    x0 = max(box1[0], box2[0])\n",
        "    y0 = max(box1[1], box2[1])\n",
        "    x1 = min(box1[2], box2[2])\n",
        "    y1 = min(box1[3], box2[3])\n",
        "    return max(0, x1 - x0) * max(0, y1 - y0)\n",
        "\n",
        "def assign_label_for_box(box, boxes, label_type):\n",
        "    overlaps = [i for i, token_box in enumerate(boxes) if box_overlap(box, token_box) > 0]\n",
        "    overlaps = sorted(overlaps)\n",
        "    return [(idx, f\"B-{label_type}\" if j == 0 else f\"I-{label_type}\") for j, idx in enumerate(overlaps)]\n",
        "\n",
        "def iob_from_kvps(words, boxes, kvps):\n",
        "    labels = [\"O\"] * len(words)\n",
        "    for kvp in kvps:\n",
        "        if \"key\" in kvp and \"bbox\" in kvp[\"key\"]:\n",
        "            for idx, tag in assign_label_for_box(kvp[\"key\"][\"bbox\"], boxes, \"KEY\"):\n",
        "                labels[idx] = tag\n",
        "        if \"value\" in kvp and \"bbox\" in kvp[\"value\"]:\n",
        "            for idx, tag in assign_label_for_box(kvp[\"value\"][\"bbox\"], boxes, \"VALUE\"):\n",
        "                labels[idx] = tag\n",
        "    return labels\n",
        "\n",
        "# === Eksempel-laster ===\n",
        "def load_example(doc_id, base_path):\n",
        "    image_path = os.path.join(base_path, \"images\", f\"{doc_id}.png\")\n",
        "    ocr_path = os.path.join(base_path, \"ocrs\", f\"{doc_id}.json\")\n",
        "    gt_path = os.path.join(base_path, \"gts\", f\"{doc_id}.json\")\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    with open(ocr_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        ocr_data = json.load(f)\n",
        "    with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        gt_data = json.load(f)\n",
        "\n",
        "    page = ocr_data[\"pages\"][0]\n",
        "    width, height = page[\"width\"], page[\"height\"]\n",
        "    words = [w[\"text\"] for w in page[\"words\"]]\n",
        "    raw_boxes = [w[\"bbox\"] for w in page[\"words\"]]\n",
        "    boxes = [normalize_bbox(b, width, height) for b in raw_boxes]\n",
        "    string_labels = iob_from_kvps(words, raw_boxes, gt_data[\"kvps_list\"])\n",
        "\n",
        "    return words, boxes, [label_map[l] for l in string_labels], image\n",
        "\n",
        "# === Prepare-funksjon for batching ===\n",
        "def prepare_examples(examples):\n",
        "    images = examples[\"image\"]\n",
        "    words = examples[\"tokens\"]\n",
        "    boxes = examples[\"bboxes\"]\n",
        "    word_labels = examples[\"ner_tags\"]\n",
        "\n",
        "    encoding = processor(\n",
        "        images,\n",
        "        words,\n",
        "        boxes=boxes,\n",
        "        word_labels=word_labels,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": encoding[\"input_ids\"].tolist(),\n",
        "      \"attention_mask\": encoding[\"attention_mask\"].tolist(),\n",
        "      \"bbox\": encoding[\"bbox\"].tolist(),\n",
        "      \"labels\": encoding[\"labels\"].tolist(),\n",
        "      \"pixel_values\": encoding[\"pixel_values\"].tolist(),\n",
        "}\n",
        "\n",
        "features = Features({\n",
        "    \"pixel_values\": Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
        "    \"input_ids\": Sequence(Value(\"int64\")),\n",
        "    \"attention_mask\": Sequence(Value(\"int64\")),\n",
        "    \"bbox\": Array2D(dtype=\"int64\", shape=(512, 4)),\n",
        "    \"labels\": Sequence(Value(\"int64\")),\n",
        "})\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s3R_SkgWvAlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Viktig!\n",
        "Dette er grunnen til at vi bruker **gts-mappen** for å finne dokumenter for KVP-modellering. Vi ser at:\n",
        "- 📄 Antall treningsdokumenter: 7843\n",
        "- 🧪 Antall testdokumenter: 828\n",
        "Dette betyr altså at ikke alle dokumenter i KVP10k-datasettet er beregnet for KVP-predikering.\n",
        "gts-mappen er den som faktisk inneholder ground truth-annotasjoner for NER (KEY/VALUE)"
      ],
      "metadata": {
        "id": "TdnRopDQyFlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/DAT255/KVP10k-dataset/kvp10k\"\n",
        "\n",
        "train_gt_dir = os.path.join(base_path, \"train\", \"gts\")\n",
        "test_gt_dir = os.path.join(base_path, \"test\", \"gts\")\n",
        "\n",
        "num_train_docs = len([f for f in os.listdir(train_gt_dir) if f.endswith(\".json\")])\n",
        "num_test_docs = len([f for f in os.listdir(test_gt_dir) if f.endswith(\".json\")])\n",
        "\n",
        "print(f\"📄 Antall treningsdokumenter: {num_train_docs}\")\n",
        "print(f\"🧪 Antall testdokumenter: {num_test_docs}\")"
      ],
      "metadata": {
        "id": "3ShObq59xGZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
        "from datasets import Features, Sequence, Value, Array2D, Array3D, Image as HFImage\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "import json\n",
        "#JUSTER DISSE\n",
        "NUM_TRAIN_DOCS = 7843\n",
        "NUM_TEST_DOCS = 828\n",
        "\n",
        "# === Batch-loader for gitte IDer\n",
        "def load_examples_from_ids(doc_ids, split_path):\n",
        "    examples = []\n",
        "    for doc_id in tqdm(doc_ids, desc=\"Laster batch\"):\n",
        "        try:\n",
        "            image_path = os.path.join(split_path, \"images\", f\"{doc_id}.png\")\n",
        "            ocr_path = os.path.join(split_path, \"ocrs\", f\"{doc_id}.json\")\n",
        "            gt_path = os.path.join(split_path, \"gts\", f\"{doc_id}.json\")\n",
        "\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            with open(ocr_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                ocr_data = json.load(f)\n",
        "            with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                gt_data = json.load(f)\n",
        "\n",
        "            page = ocr_data[\"pages\"][0]\n",
        "            width, height = page[\"width\"], page[\"height\"]\n",
        "            words = [w[\"text\"] for w in page[\"words\"]]\n",
        "            raw_boxes = [w[\"bbox\"] for w in page[\"words\"]]\n",
        "            boxes = [normalize_bbox(b, width, height) for b in raw_boxes]\n",
        "            string_labels = iob_from_kvps(words, raw_boxes, gt_data[\"kvps_list\"])\n",
        "            label_ids = [label_map[l] for l in string_labels]\n",
        "\n",
        "            examples.append({\n",
        "                \"id\": doc_id,\n",
        "                \"tokens\": words,\n",
        "                \"bboxes\": boxes,\n",
        "                \"ner_tags\": label_ids,\n",
        "                \"image\": image\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Feil i {doc_id}: {e}\")\n",
        "\n",
        "    return Dataset.from_list(examples).cast_column(\"image\", HFImage(decode=True))\n",
        "\n",
        "\n",
        "# === Batchvis prosessering og lagring\n",
        "train_path = os.path.join(base_path, \"train\")\n",
        "gts_dir = os.path.join(train_path, \"gts\")\n",
        "doc_ids = sorted(f.replace(\".json\", \"\") for f in os.listdir(gts_dir) if f.endswith(\".json\"))\n",
        "doc_ids = doc_ids[:NUM_TRAIN_DOCS]\n",
        "\n",
        "batch_size = 500\n",
        "processed_batches = []\n",
        "\n",
        "for i in range(0, len(doc_ids), batch_size):\n",
        "    print(f\"\\n🔁 Behandler batch {i // batch_size + 1}\")\n",
        "    batch_ids = doc_ids[i:i+batch_size]\n",
        "\n",
        "    raw = load_examples_from_ids(batch_ids, train_path)\n",
        "    processed = raw.map(\n",
        "        prepare_examples,\n",
        "        batched=True,\n",
        "        batch_size=32,\n",
        "        remove_columns=[\"tokens\", \"bboxes\", \"image\", \"ner_tags\", \"id\"],\n",
        "        features=features,\n",
        "        load_from_cache_file=True\n",
        "    )\n",
        "    processed_batches.append(processed)\n",
        "\n",
        "# === Slå sammen og splitt\n",
        "full_train_dataset = concatenate_datasets(processed_batches)\n",
        "split = full_train_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# === Load og prosesser test-settet separat\n",
        "test_path = os.path.join(base_path, \"test\")\n",
        "gts_test_dir = os.path.join(test_path, \"gts\")\n",
        "test_doc_ids = sorted(f.replace(\".json\", \"\") for f in os.listdir(gts_test_dir) if f.endswith(\".json\"))\n",
        "test_doc_ids = test_doc_ids[:NUM_TEST_DOCS]\n",
        "\n",
        "raw_test = load_examples_from_ids(test_doc_ids, test_path)\n",
        "test_dataset = raw_test.map(\n",
        "    prepare_examples,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    remove_columns=[\"tokens\", \"bboxes\", \"image\", \"ner_tags\", \"id\"],\n",
        "    features=features,\n",
        "    desc=\"Preprosesserer test\",\n",
        "    load_from_cache_file=True\n",
        ")\n",
        "\n",
        "# === Lag og lagre hele datasettet\n",
        "final_dataset = DatasetDict({\n",
        "    \"train\": split[\"train\"],\n",
        "    \"eval\": split[\"test\"],\n",
        "    \"test\": test_dataset\n",
        "})"
      ],
      "metadata": {
        "id": "cjCeic1a71tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/KVP10k_processed_ready/dataset_all_gts\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "final_dataset.save_to_disk(output_dir)\n"
      ],
      "metadata": {
        "id": "xmeWY3FNyk4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh \"/content/drive/MyDrive/KVP10k_processed_ready\"\n"
      ],
      "metadata": {
        "id": "ZmYHsX2LOUW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/drive/MyDrive/KVP10k_processed_ready/dataset\n"
      ],
      "metadata": {
        "id": "QgId3QbwOyEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Monter Google Drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Slett eksisterende mappe hvis den finnes\n",
        "output_path = \"/content/drive/MyDrive/KVP10k_processed_ready\"\n",
        "shutil.rmtree(output_path, ignore_errors=True)\n",
        "\n",
        "# Lagre til lokal disk først\n",
        "local_tmp_path = \"/content/KVP10k_tmp\"\n",
        "shutil.rmtree(local_tmp_path, ignore_errors=True)\n",
        "final_dataset.save_to_disk(local_tmp_path)\n",
        "\n",
        "# Kopier til Drive\n",
        "shutil.copytree(local_tmp_path, output_path)\n",
        "\n",
        "# Verifiser\n",
        "print(\"✅ Datasett kopiert til Drive:\")\n",
        "!ls -lh \"$output_path\"\n"
      ],
      "metadata": {
        "id": "tt2Bj_LWQu8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "def tell_labels(dataset, label_column=\"labels\", label_map=label_map):\n",
        "    all_labels = list(chain.from_iterable(dataset[label_column]))\n",
        "    label_counts = Counter(all_labels)\n",
        "    inv_map = {v: k for k, v in label_map.items()}\n",
        "    named_counts = {inv_map.get(k, f\"UNK({k})\"): v for k, v in label_counts.items()}\n",
        "    return named_counts\n",
        "\n",
        "print(\"Train labels:\", tell_labels(final_dataset[\"train\"]))\n"
      ],
      "metadata": {
        "id": "9Y8vGYx49KIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = final_dataset[\"train\"][0]\n",
        "print(\"Tokens:\", len(example[\"input_ids\"]))\n",
        "print(\"Labels:\", len(example[\"labels\"]))\n"
      ],
      "metadata": {
        "id": "l1oblabZ9RYX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}