{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/667029/KVP10k/blob/main/layoutlmv3_kvp10k_relation_layer_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets torch transformers accelerate numpy tqdm tensorflow scikit-learn"
      ],
      "metadata": {
        "id": "d-7H4vd91nif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definerer drive lokasjon\n",
        "drive_mount_path='/content/drive'\n",
        "\n",
        "# Definerer hidden state kontanter\n",
        "hidden_size = 768"
      ],
      "metadata": {
        "id": "xZ2_PEVbhQ7m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_from_disk\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DataUtil:\n",
        "    @staticmethod\n",
        "    def get_output(dataset_path=None, model_output=None, drive_mount_path=drive_mount_path,\n",
        "                   extract_hidden=False, verbose=True):\n",
        "        \"\"\"\n",
        "        Loads a Hugging Face dataset from disk (if dataset_path is provided) or returns\n",
        "        a given model_output.\n",
        "        \"\"\"\n",
        "        if dataset_path is not None:\n",
        "            try:\n",
        "                from google.colab import drive\n",
        "                drive.mount(drive_mount_path, force_remount=False)\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(\"Google Drive may already be mounted. Continuing...\")\n",
        "            dataset = load_from_disk(dataset_path)\n",
        "            if verbose:\n",
        "                print(\"Loaded dataset from drive.\")\n",
        "                print(\"Number of samples:\", len(dataset))\n",
        "                print(\"Column names:\", dataset.column_names)\n",
        "                print(\"First sample:\", dataset[0])\n",
        "            return dataset\n",
        "        elif model_output is not None:\n",
        "            if extract_hidden:\n",
        "                if isinstance(model_output, dict) and \"hidden_states\" in model_output:\n",
        "                    model_output = model_output[\"hidden_states\"]\n",
        "                elif isinstance(model_output, (list, tuple)):\n",
        "                    model_output = model_output[-1]\n",
        "            if verbose:\n",
        "                print(\"Provided model output:\")\n",
        "                print(model_output)\n",
        "            return model_output\n",
        "        else:\n",
        "            raise ValueError(\"Please provide either a dataset_path or a model_output.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def to_tf_dataset(dataset, hidden_size, batch_size=32, shuffle_buffer=1000, verbose=True):\n",
        "        \"\"\"\n",
        "        Converts a Hugging Face dataset into a tf.data.Dataset.\n",
        "        Assumes each sample contains keys \"h_i\", \"h_j\", and \"label\".\n",
        "        If the label is a sequence, the first element is used.\n",
        "        Assumes that \"h_i\" and \"h_j\" are 1D vectors (of length hidden_size).\n",
        "        \"\"\"\n",
        "        def generator():\n",
        "            for sample in dataset:\n",
        "                h_i = np.array(sample[\"h_i\"], dtype=np.float32)  # shape: (hidden_size,)\n",
        "                h_j = np.array(sample[\"h_j\"], dtype=np.float32)  # shape: (hidden_size,)\n",
        "                label_arr = np.array(sample[\"label\"], dtype=np.int32)\n",
        "                label = label_arr[0] if label_arr.ndim > 0 else label_arr\n",
        "                yield (h_i, h_j, label)\n",
        "\n",
        "        # Use first sample to determine shape.\n",
        "        first_sample = dataset[0]\n",
        "        h_i_first = np.array(first_sample[\"h_i\"], dtype=np.float32)\n",
        "        if h_i_first.ndim != 1:\n",
        "            raise ValueError(\"Expected h_i to be a 1D vector; got shape \" + str(h_i_first.shape))\n",
        "        output_signature = (\n",
        "            tf.TensorSpec(shape=(hidden_size,), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(hidden_size,), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "\n",
        "        tf_dataset = tf.data.Dataset.from_generator(generator, output_signature=output_signature)\n",
        "        tf_dataset = tf_dataset.shuffle(shuffle_buffer).batch(batch_size)\n",
        "        if verbose:\n",
        "            print(f\"Converted dataset to tf.data.Dataset with batch size {batch_size} and h_i shape {(hidden_size,)}.\")\n",
        "        return tf_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "Poz9GgAvWYTp",
        "outputId": "8a0b030a-b25f-4e31-c090-3c9081d4ed0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0d64ea2b7812>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. # Hent dataset fra drive\n",
        "dataset_path = \"/content/drive/MyDrive/RE_ready/re_dataset_test_combined\"\n",
        "loaded_dataset = DataUtil.get_output(dataset_path=dataset_path)\n",
        "\n",
        "\n",
        "# 2. # Hent dataset fra output av layoutmvl3 modellen\n",
        "# model_output = outputs\n",
        "# dataset = DataUtil.save_output_as_dataset(model_output=model_output, verbose=True)\n"
      ],
      "metadata": {
        "id": "NQN3xQfhL-KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Opprett dataset laster\n",
        "train_dataset_tf = DataUtil.to_tf_dataset(loaded_dataset, hidden_size=hidden_size, batch_size=32)\n",
        "\n",
        "# For visuel verifisering av datasetets oppbygging stemmer med forventninger\n",
        "for batch in train_dataset_tf.take(1):\n",
        "    h_i_batch, h_j_batch, labels_batch = batch\n",
        "    print(\"h_i batch shape:\", h_i_batch.shape)  # Expect (batch_size, 768)\n",
        "    print(\"h_j batch shape:\", h_j_batch.shape)  # Expect (batch_size, 768)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)  # Expect (batch_size,)"
      ],
      "metadata": {
        "id": "qpfHKhtSl2lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opprett dataset batches\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(32)"
      ],
      "metadata": {
        "id": "OHwAvAT1Z-0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the relation extraction model as a subclass of tf.keras.Model.\n",
        "class RelationExtractionHead(tf.keras.Model):\n",
        "    def __init__(self, hidden_size, num_relations):\n",
        "        super(RelationExtractionHead, self).__init__()\n",
        "        self.head_proj = tf.keras.layers.Dense(hidden_size, activation='relu', name=\"head_proj\")\n",
        "        self.tail_proj = tf.keras.layers.Dense(hidden_size, activation='relu', name=\"tail_proj\")\n",
        "        self.combined_dense = tf.keras.layers.Dense(num_relations, name=\"combined_dense\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        h_i, h_j = inputs  # Each: (batch, hidden_size)\n",
        "        head = self.head_proj(h_i)\n",
        "        tail = self.tail_proj(h_j)\n",
        "        # Element-wise multiplication.\n",
        "        x = head * tail\n",
        "        logits = self.combined_dense(x)  # Output shape: (batch, num_relations)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "FmOXaQT9mjMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tqdm import tqdm  # For progress visualization.\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assume these hyperparameters are set:\n",
        "num_relations = 5       # Number of relation classes.\n",
        "num_epochs = 2          # Number of training epochs.\n",
        "learning_rate = 1e-4    # Learning rate.\n",
        "hidden_size = 768       # Size of the hidden vectors.\n",
        "batch_size = 32\n",
        "\n",
        "# Define the model (RelationExtractionHead) (using the updated Bilinear layer).\n",
        "model_tf = RelationExtractionHead(hidden_size=hidden_size, num_relations=num_relations)\n",
        "\n",
        "# Define loss function and optimizer.\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Assume train_dataset is your tf.data.Dataset (created earlier by DataUtil.to_tf_dataset).\n",
        "# Compute total number of batches from the original loaded dataset.\n",
        "# Assume \"loaded_dataset\" is the Hugging Face dataset loaded using DataUtil.get_output.\n",
        "num_samples = len(loaded_dataset)\n",
        "total_batches = int(np.ceil(num_samples / batch_size))\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    batches = 0\n",
        "    progress_bar = tqdm(train_dataset, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\", total=total_batches)\n",
        "    for h_i_batch, h_j_batch, labels_batch in progress_bar:\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model_tf((h_i_batch, h_j_batch), training=True)\n",
        "            loss_value = loss_fn(labels_batch, logits)\n",
        "        grads = tape.gradient(loss_value, model_tf.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model_tf.trainable_variables))\n",
        "        epoch_loss += loss_value.numpy()\n",
        "        batches += 1\n",
        "        progress_bar.set_postfix(loss=loss_value.numpy())\n",
        "    avg_loss = epoch_loss / batches\n",
        "    loss_history.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, num_epochs+1), loss_history, marker='o', linestyle='-')\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s3ISuPinMAi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "class RelationModelTesterTF:\n",
        "    def __init__(self, model, dataset, batch_size=32):\n",
        "        self.model = model\n",
        "        self.dataset = dataset.batch(batch_size)\n",
        "\n",
        "    def evaluate(self):\n",
        "        all_predictions = []\n",
        "        all_ground_truths = []\n",
        "        for h_i_batch, h_j_batch, labels_batch in self.dataset:\n",
        "            logits = self.model((h_i_batch, h_j_batch), training=False)\n",
        "            preds = tf.argmax(logits, axis=-1)\n",
        "            all_predictions.extend(preds.numpy().tolist())\n",
        "            all_ground_truths.extend(labels_batch.numpy().tolist())\n",
        "        all_predictions = np.array(all_predictions)\n",
        "        all_ground_truths = np.array(all_ground_truths)\n",
        "        accuracy = np.mean(all_predictions == all_ground_truths)\n",
        "        report = classification_report(all_ground_truths, all_predictions, digits=4)\n",
        "        print(\"Classification Report:\\n\", report)\n",
        "        print(\"Overall Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "        return {\"predictions\": all_predictions, \"ground_truth\": all_ground_truths, \"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "ftwY38OpUYze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tester_tf = RelationModelTesterTF(model_tf, train_dataset, batch_size=32)\n",
        "results_tf = tester_tf.evaluate()\n",
        "\n",
        "print(\"Test Accuracy: {:.2f}%\".format(results_tf[\"accuracy\"] * 100))\n",
        "print(\"Example Predictions (first 10 samples):\", results_tf[\"predictions\"][:10])"
      ],
      "metadata": {
        "id": "Jlkzukx1UbLB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}