{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/667029/KVP10k/blob/main/LayoutLMv3_KVP10k_9april_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbpDvQ26mMOL"
      },
      "source": [
        "_______"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9kWf_R2vNou"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is3Rapoauqj1"
      },
      "outputs": [],
      "source": [
        "#!jupyter nbconvert --ClearOutputPreprocessor.enabled=True \\\n",
        "#  --inplace \"/content/drive/MyDrive/Colab Notebooks/LayoutLMv3_KVP10k_9april.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TqejcrWXvnoL"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av-n3h6jaou3"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE27SIYc2Y2y"
      },
      "outputs": [],
      "source": [
        "import os              #navigere mapper og filer, hente filbaner\n",
        "from PIL import Image  #åpne, vise og manipulere bilder\n",
        "import json            #lese/skrive til JSON-filer\n",
        "from transformers import LayoutLMv3Processor\n",
        "import torch\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Uup0eV5_usd"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW4eqj9o2trD"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/DAT255/KVP10k-dataset/kvp10k/\"\n",
        "print(os.listdir(base_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYm7ncZw_lSW"
      },
      "outputs": [],
      "source": [
        "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False) # <-- Viktig fordi vi allerede har utført OCR på bildet og har tekst og bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsRz3cC2_8mj"
      },
      "outputs": [],
      "source": [
        "# Mapping fra tekstlige BIO-labels til tall som modellen bruker\n",
        "label_map = {\n",
        "    \"O\": 0,\n",
        "    \"B-KEY\": 1,\n",
        "    \"I-KEY\": 2,\n",
        "    \"B-VALUE\": 3,\n",
        "    \"I-VALUE\": 4,\n",
        "}\n",
        "\n",
        "# Funksjon for å skalere bounding boxes til 0-1000 (som LayoutLMv3 krever)\n",
        "def normalize_bbox(bbox, width, height):\n",
        "  return [\n",
        "      int(1000 * (bbox[0] /width)),\n",
        "      int(1000 * (bbox[1] / height)),\n",
        "      int(1000 * (bbox[2] / width)),\n",
        "      int(1000 * (bbox[3] / height))\n",
        "  ]\n",
        "\n",
        "\n",
        "def assign_label_for_box(box, boxes, label_type):\n",
        "  \"\"\"Returnerer liste med (index, label) for tokens som overlapper box\"\"\"\n",
        "  overlaps = []\n",
        "  for i, token_box in enumerate(boxes):\n",
        "    if box_overlap(box, token_box) > 0:\n",
        "      overlaps.append(i)\n",
        "\n",
        "  overlaps = sorted(overlaps)\n",
        "\n",
        "  labeled = []\n",
        "  for j, idx in enumerate(overlaps):\n",
        "    tag = f\"B-{label_type}\" if j == 0 else f\"I-{label_type}\"\n",
        "    labeled.append((idx, tag))\n",
        "\n",
        "  return labeled\n",
        "\n",
        "\n",
        "#Sjekker om OCR-boksen overlapper med GTS(key/value)-boksen.\n",
        "#Ved overlapp hører de til hverandre.\n",
        "def box_overlap(box1, box2):\n",
        "  x0 = max(box1[0], box2[0])\n",
        "  y0 = max(box1[1], box2[1])\n",
        "  x1 = min(box1[2], box2[2])\n",
        "  y1 = min(box1[3], box2[3])\n",
        "  return max(0, x1 - x0) * max(0, y1 - y0)\n",
        "\n",
        "\n",
        "# Funksjon for å generere BIO-labels fra gts (ground truth).\n",
        "# Lager en BIO-label for hvert token basert på om det overlapper med en key- eller value-boks fra GTS.\n",
        "# Matcher hvert token fra OCR (word + bbox) mot key/value-bbokser fra gts:\n",
        "# --> Token overlapper en nøkkelboks: B-KEY eller I-KEY\n",
        "# --> Token overlapper en verdiboks: B-VALUE eller I-VALUE\n",
        "# --> Ellers: O\n",
        "def iob_from_kvps(words, boxes, kvps):\n",
        "  labels = [\"O\"] * len(words)\n",
        "\n",
        "  #Gå igjennom alle key-value-pairs\n",
        "  for kvp in kvps:\n",
        "    if \"key\" in kvp and \"bbox\" in kvp[\"key\"]:\n",
        "      key_bbox = kvp[\"key\"][\"bbox\"]\n",
        "      for idx, tag in assign_label_for_box(key_bbox, boxes, \"KEY\"):\n",
        "        labels[idx] = tag\n",
        "\n",
        "    if \"value\" in kvp and \"bbox\" in kvp[\"value\"]:\n",
        "      value_box = kvp[\"value\"][\"bbox\"]\n",
        "      for idx, tag in assign_label_for_box(value_box, boxes, \"VALUE\"):\n",
        "        labels[idx] = tag\n",
        "\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5PStee23zyR"
      },
      "outputs": [],
      "source": [
        "def load_example(doc_id, base_path):\n",
        "  image_path = os.path.join(base_path, \"images\", f\"{doc_id}.png\")\n",
        "  ocr_path = os.path.join(base_path, \"ocrs\", f\"{doc_id}.json\")\n",
        "  gt_path = os.path.join(base_path, \"gts\", f\"{doc_id}.json\")\n",
        "\n",
        "  image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "  with open(ocr_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    ocr_data = json.load(f)\n",
        "\n",
        "  with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    gt_data = json.load(f)\n",
        "\n",
        "  page = ocr_data[\"pages\"][0]\n",
        "  width, height = page[\"width\"], page[\"height\"]\n",
        "\n",
        "  words = [w[\"text\"] for w in page[\"words\"]]\n",
        "  raw_boxes = [w[\"bbox\"] for w in page[\"words\"]]\n",
        "  boxes = [normalize_bbox(b, width, height) for b in raw_boxes]\n",
        "\n",
        "  kvps = gt_data[\"kvps_list\"]\n",
        "  string_labels = iob_from_kvps(words, raw_boxes, kvps)\n",
        "  labels = [label_map[l] for l in string_labels]\n",
        "\n",
        "  #Fyller på med padding, og truncation klipper av hvis sekvensen har for mange tokens, returnerer som PyTorch-tensor\n",
        "  encoding = processor(image, words, boxes=boxes, word_labels=labels, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  return encoding, words, boxes, string_labels, image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap9FKvFRBEev"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict, Image as HFImage\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def load_split(split_name, base_path, limit=None):\n",
        "    split_path = os.path.join(base_path, split_name)\n",
        "    gts_dir = os.path.join(split_path, \"gts\")\n",
        "\n",
        "    doc_ids = sorted([\n",
        "        fname.replace(\".json\", \"\")\n",
        "        for fname in os.listdir(gts_dir)\n",
        "        if fname.endswith(\".json\")\n",
        "    ])\n",
        "\n",
        "    if limit:\n",
        "        doc_ids = doc_ids[:limit]\n",
        "\n",
        "    examples = []\n",
        "\n",
        "    for doc_id in tqdm(doc_ids, desc=f\"Laster {split_name}\"):\n",
        "        try:\n",
        "            encoding, words, boxes, string_labels, image = load_example(doc_id, split_path)\n",
        "\n",
        "            example = {\n",
        "                \"id\": doc_id,\n",
        "                \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "                \"bbox\": encoding[\"bbox\"].squeeze(0),\n",
        "                \"labels\": encoding[\"labels\"].squeeze(0),\n",
        "                \"pixel_values\": encoding[\"pixel_values\"].squeeze(0),\n",
        "                \"image\": image,\n",
        "                \"tokens\": words,\n",
        "                \"bboxes\": boxes,\n",
        "                \"ner_tags\": [label_map[l] for l in string_labels]\n",
        "\n",
        "            }\n",
        "\n",
        "            examples.append(example)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Feil i {doc_id}: {e}\")\n",
        "\n",
        "    #return Dataset.from_list(examples)\n",
        "    return Dataset.from_list(examples).cast_column(\"image\", HFImage(decode=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3ZQO0JOwlZy"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "# Stien du lagret til\n",
        "dataset = load_from_disk(\"/content/drive/MyDrive/KVP10k_processed_ready/dataset\")\n",
        "\n",
        "# Hent delene\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"eval\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "train_dataset.set_format(\"torch\")\n",
        "eval_dataset.set_format(\"torch\")\n",
        "test_dataset.set_format(\"torch\")\n",
        "\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Eval size:\", len(eval_dataset))\n",
        "print(\"Test size:\", len(test_dataset))\n",
        "\n",
        "# Eksempel på batch-format\n",
        "print(train_dataset[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iFK9Np5BUJpX"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# Last inn begrenset antall dokumenter for utvikling/testing\n",
        "raw_train_dataset = load_split(\"train\", base_path, limit=1000)\n",
        "raw_test_dataset = load_split(\"test\", base_path, limit=100)\n",
        "\n",
        "# Kombiner i Hugging Face-format\n",
        "dataset = DatasetDict({\n",
        "    \"train\": deepcopy(raw_train_dataset),\n",
        "    \"test\": deepcopy(raw_test_dataset),\n",
        "})\n",
        "\n",
        "# Angi hvilke kolonner som skal konverteres til PyTorch-tensorer\n",
        "dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"bbox\", \"labels\", \"pixel_values\"],\n",
        ")\n",
        "\n",
        "# Splitt treningssettet i train + eval (f.eks. 80/20)\n",
        "split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = split[\"train\"]\n",
        "eval_dataset = split[\"test\"]\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Fullt datasett: {dataset}\")\n",
        "print(f\"Train: {train_dataset}\")\n",
        "print(f\"Eval: {eval_dataset}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya332BhmGLt_"
      },
      "outputs": [],
      "source": [
        "train_dataset.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EB3kZmZpTu-m"
      },
      "outputs": [],
      "source": [
        "example = train_dataset[0]\n",
        "for k,v in example.items():\n",
        "    print(k,v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GTN1cXBYTynu"
      },
      "outputs": [],
      "source": [
        "processor.tokenizer.decode(train_dataset[0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hhU1sClvWGOS"
      },
      "outputs": [],
      "source": [
        "for id, label in zip(train_dataset[0][\"input_ids\"], train_dataset[0][\"labels\"]):\n",
        "  print(processor.tokenizer.decode([id]), label.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmYZ6LnbaWTv"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "metric = load(\"seqeval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nAjhdhwa1fh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "label_list = [\"O\", \"B-KEY\", \"I-KEY\", \"B-VALUE\", \"I-VALUE\"]\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DzA5KFAdeA8"
      },
      "outputs": [],
      "source": [
        "from transformers import LayoutLMv3ForTokenClassification\n",
        "\n",
        "model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\",\n",
        "                                                         id2label=id2label,\n",
        "                                                         label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg8bMcM5dmYW"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    output_dir=\"test\",\n",
        "    num_train_epochs=8,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-6,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=125,\n",
        "    save_steps=250,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"tensorboard\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,\n",
        "    fp16=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "endgETwXet1h"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, EarlyStoppingCallback\n",
        "from transformers.data.data_collator import default_data_collator\n",
        "\n",
        "trainer = Trainer (\n",
        "    model=model,\n",
        "    args=train_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5erU_T1xfOLw"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e36vlg1KYwau"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "judb3GpS6o4Y"
      },
      "outputs": [],
      "source": [
        "# Angi en mappe i Drive (eller lokalt hvis du vil kopiere senere)\n",
        "output_dir = \"/content/drive/MyDrive/layoutlmv3_kvp10k_model\"\n",
        "\n",
        "# Lagre modell og tokenizer\n",
        "trainer.save_model(output_dir)\n",
        "processor.save_pretrained(output_dir)  # dette lagrer både tokenizer + feature extracto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9m6YoQibKSv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"/content/test/checkpoint-500\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbNiMYEGdon9"
      },
      "outputs": [],
      "source": [
        "example = dataset[\"test\"][2]\n",
        "print(example.keys())\n",
        "example_raw = raw_test_dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_q4Tox9hmWz"
      },
      "outputs": [],
      "source": [
        "for k, v in example.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt2YgixOic3B"
      },
      "outputs": [],
      "source": [
        "# Pakker ut tensorene\n",
        "inputs = {\n",
        "    \"input_ids\": example[\"input_ids\"].unsqueeze(0),\n",
        "    \"attention_mask\": example[\"attention_mask\"].unsqueeze(0),\n",
        "    \"bbox\": example[\"bbox\"].unsqueeze(0),\n",
        "    \"pixel_values\": example[\"pixel_values\"].unsqueeze(0)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaP1O8hhk3Sw"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy3B2ai6k6I3"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fe1uVF7k8wF"
      },
      "outputs": [],
      "source": [
        "logits = outputs.logits\n",
        "predictions = logits.argmax(-1).squeeze().tolist()\n",
        "labels = example[\"labels\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwq-E8txlEqY"
      },
      "outputs": [],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oDqmq_zlK5E"
      },
      "outputs": [],
      "source": [
        "print(f\"{'Token ID':10} {'Label':10} {'Pred':10}\")\n",
        "print(\"=\" * 30)\n",
        "for token_id, label_id, pred_id in zip(example[\"input_ids\"], labels, predictions):\n",
        "    if label_id == -100:\n",
        "        continue\n",
        "    token = processor.tokenizer.decode([token_id])\n",
        "    print(f\"{token:10} {id2label[label_id.item()]:10} {id2label[pred_id]:10}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P5bE26CmCxD"
      },
      "outputs": [],
      "source": [
        "def unnormalize_box(bbox, width, height):\n",
        "    return [\n",
        "        width * (bbox[0] / 1000),\n",
        "        height * (bbox[1] / 1000),\n",
        "        width * (bbox[2] / 1000),\n",
        "        height * (bbox[3] / 1000),\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrQL3-p6mEU_"
      },
      "outputs": [],
      "source": [
        "image = example_raw[\"image\"].copy().convert(\"RGB\")  # PIL image\n",
        "token_boxes = example[\"bbox\"]  # allerede ferdig prosessert\n",
        "labels = example[\"labels\"]\n",
        "input_ids = example[\"input_ids\"]\n",
        "\n",
        "# Unnormalize bboxes\n",
        "width, height = image.size\n",
        "true_boxes = [\n",
        "    unnormalize_box(box, width, height)\n",
        "    for box, label in zip(token_boxes, labels)\n",
        "    if label != -100\n",
        "]\n",
        "\n",
        "\n",
        "# Konverter til tekst og fjern -100 padding\n",
        "true_predictions = [\n",
        "    id2label[int(pred)] for pred, label in zip(predictions, labels) if label != -100\n",
        "]\n",
        "true_labels = [\n",
        "    id2label[int(label)] for pred, label in zip(predictions, labels) if label != -100\n",
        "]\n",
        "\n",
        "token_boxes = example[\"bbox\"]\n",
        "width, height = example_raw[\"image\"].size\n",
        "\n",
        "true_boxes = [\n",
        "    unnormalize_box(box, width, height)\n",
        "    for box, label in zip(token_boxes, labels)\n",
        "    if label != -100\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DACjGfDhmbp3"
      },
      "outputs": [],
      "source": [
        "def iob_to_label(label):\n",
        "    if label.startswith(\"B-\") or label.startswith(\"I-\"):\n",
        "        return label[2:].lower()\n",
        "    return \"other\"\n",
        "\n",
        "label2color = {\n",
        "    \"key\": \"blue\",\n",
        "    \"value\": \"green\",\n",
        "    \"other\": \"gray\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOLcBpKEATBn"
      },
      "source": [
        "Models predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYkOi5hFmlsL"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageDraw, ImageFont\n",
        "from IPython.display import display\n",
        "\n",
        "draw = ImageDraw.Draw(image)\n",
        "font = ImageFont.load_default()\n",
        "\n",
        "for pred_label, box in zip(true_predictions, true_boxes):\n",
        "    label = iob_to_label(pred_label)\n",
        "    draw.rectangle(box, outline=label2color.get(label, \"red\"), width=2)\n",
        "    draw.text((box[0] + 10, box[1] - 10), label, fill=label2color.get(label, \"red\"), font=font)\n",
        "\n",
        "display(image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN7kkyH8AOYP"
      },
      "source": [
        "_______\n",
        "Ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51xKEuUcmoLu"
      },
      "outputs": [],
      "source": [
        "image_true = example_raw[\"image\"].convert(\"RGB\")\n",
        "draw_true = ImageDraw.Draw(image_true)\n",
        "\n",
        "for word, box, label_id in zip(example_raw['tokens'], example_raw['bboxes'], example_raw['ner_tags']):\n",
        "    label = iob_to_label(id2label[label_id]).lower()\n",
        "    box = unnormalize_box(box, width, height)\n",
        "    draw_true.rectangle(box, outline=label2color.get(label, \"gray\"), width=2)\n",
        "    draw_true.text((box[0] + 10, box[1] - 10), label, fill=label2color.get(label, \"gray\"), font=font)\n",
        "\n",
        "display(image_true)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}