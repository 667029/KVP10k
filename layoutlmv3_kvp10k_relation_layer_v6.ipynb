{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/667029/KVP10k/blob/main/layoutlmv3_kvp10k_relation_layer_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdFTecmPN2ix",
        "outputId": "45cba205-96e6-40c7-b894-268984d090d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m236.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install datasets torch transformers accelerate numpy tqdm tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ2_PEVbhQ7m"
      },
      "outputs": [],
      "source": [
        "# Definerer drive lokasjon\n",
        "drive_mount_path='/content/drive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiqiSaOt22RY"
      },
      "outputs": [],
      "source": [
        "hidden_size = 768           # Size of the hidden vectors.\n",
        "batch_size = 32             # Batch size.\n",
        "test_size_percentage = 10   # Percentage of dataset to be dedicated to test\n",
        "seed = 42                   # Seed for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Poz9GgAvWYTp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_from_disk\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DataUtil:\n",
        "    @staticmethod\n",
        "    def get_output(dataset_path=None, model_output=None, drive_mount_path=drive_mount_path, extract_hidden=False, verbose=True):\n",
        "        if dataset_path is not None:\n",
        "            try:\n",
        "                from google.colab import drive\n",
        "                drive.mount(drive_mount_path, force_remount=False)\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(\"Google Drive may already be mounted. Continuing...\")\n",
        "            dataset = load_from_disk(dataset_path)\n",
        "            if verbose:\n",
        "                print(\"Loaded dataset from drive.\")\n",
        "                print(\"Number of samples:\", len(dataset))\n",
        "                print(\"Column names:\", dataset.column_names)\n",
        "                print(\"First sample:\", dataset[0])\n",
        "            return dataset\n",
        "        elif model_output is not None:\n",
        "            if extract_hidden:\n",
        "                if isinstance(model_output, dict) and \"hidden_states\" in model_output:\n",
        "                    model_output = model_output[\"hidden_states\"]\n",
        "                elif isinstance(model_output, (list, tuple)):\n",
        "                    model_output = model_output[-1]\n",
        "            if verbose:\n",
        "                print(\"Provided model output:\")\n",
        "                print(model_output)\n",
        "            return model_output\n",
        "        else:\n",
        "            raise ValueError(\"Please provide either a dataset_path or a model_output.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def to_tf_dataset(dataset, hidden_size, batch_size=32, shuffle_buffer=1000, verbose=True):\n",
        "        def generator():\n",
        "            for sample in dataset:\n",
        "                h_i = np.array(sample[\"h_i\"], dtype=np.float32)\n",
        "                h_j = np.array(sample[\"h_j\"], dtype=np.float32)\n",
        "                label_arr = np.array(sample[\"label\"], dtype=np.int32)\n",
        "                label = label_arr[0] if label_arr.ndim > 0 else label_arr\n",
        "                key = sample[\"key\"] if \"key\" in sample else \"unknown\"\n",
        "                yield (h_i, h_j, label, key)\n",
        "\n",
        "        first_sample = dataset[0]\n",
        "        h_i_first = np.array(first_sample[\"h_i\"], dtype=np.float32)\n",
        "\n",
        "        if h_i_first.ndim == 1:\n",
        "            output_signature = (\n",
        "                tf.TensorSpec(shape=(hidden_size,), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(hidden_size,), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.int32),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.string),\n",
        "            )\n",
        "            ds = tf.data.Dataset.from_generator(generator, output_signature=output_signature)\n",
        "            ds = ds.shuffle(shuffle_buffer).batch(batch_size)\n",
        "        elif h_i_first.ndim == 2:\n",
        "            output_signature = (\n",
        "                tf.TensorSpec(shape=(None, hidden_size), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(None, hidden_size), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.int32),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.string),\n",
        "            )\n",
        "            ds = tf.data.Dataset.from_generator(generator, output_signature=output_signature)\n",
        "            ds = ds.shuffle(shuffle_buffer).padded_batch(\n",
        "                batch_size,\n",
        "                padded_shapes=([None, hidden_size], [None, hidden_size], [], [])\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Expected h_i to be 1D or 2D; got shape \" + str(h_i_first.shape))\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Converted dataset to tf.data.Dataset with batch size {batch_size}.\")\n",
        "        return ds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQN3xQfhL-KZ"
      },
      "outputs": [],
      "source": [
        "# 1. # Hent dataset fra drive\n",
        "# Paths to your datasets\n",
        "train_dataset_path = \"/content/drive/MyDrive/RE_ready/re_dataset_train_combined\"\n",
        "test_dataset_path = \"/content/drive/MyDrive/RE_ready/re_dataset_test_combined\"\n",
        "\n",
        "# Load training dataset\n",
        "train_loaded_dataset = DataUtil.get_output(dataset_path=train_dataset_path)\n",
        "# Split into train/eval sets (e.g., 90% train, 10% eval)\n",
        "split_dataset = train_loaded_dataset.train_test_split(test_size=test_size_percentage/100, seed=seed)\n",
        "\n",
        "# Explicitly naming sets\n",
        "train_dataset_hf = split_dataset[\"train\"]           # Training set (with labels)\n",
        "eval_dataset_hf = split_dataset[\"test\"]             # Eval set (with labels for evaluation during training)\n",
        "\n",
        "# Load your separate test dataset for inference/evaluation\n",
        "test_loaded_dataset = DataUtil.get_output(dataset_path=test_dataset_path)\n",
        "test_dataset_hf = test_loaded_dataset\n",
        "\n",
        "\n",
        "\n",
        "# 2. # Hent dataset fra output av layoutmvl3 modellen\n",
        "# model_output = outputs\n",
        "# dataset = DataUtil.save_output_as_dataset(model_output=model_output, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpfHKhtSl2lT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Oppretter trening og test set\n",
        "train_dataset = DataUtil.to_tf_dataset(train_dataset_hf, hidden_size=hidden_size, batch_size=32)\n",
        "for batch in train_dataset.take(1):\n",
        "    h_i_batch, h_j_batch, labels_batch, keys_batch = batch\n",
        "    print(\"h_i batch shape:\", h_i_batch.shape)\n",
        "    print(\"h_j batch shape:\", h_j_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    print(\"keys batch shape:\", keys_batch.shape)\n",
        "\n",
        "test_dataset = DataUtil.to_tf_dataset(test_dataset_hf, hidden_size=hidden_size, batch_size=32)\n",
        "for batch in test_dataset.take(1):\n",
        "    h_i_batch, h_j_batch, labels_batch, keys_batch = batch\n",
        "    print(\"h_i batch shape:\", h_i_batch.shape)\n",
        "    print(\"h_j batch shape:\", h_j_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    print(\"keys batch shape:\", keys_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkbmBV0Ok5wz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class Bilinear(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_size, num_relations):\n",
        "        super(Bilinear, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_relations = num_relations\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create weight tensor of shape (hidden_size, num_relations, hidden_size)\n",
        "        self.W = self.add_weight(\n",
        "            shape=(self.hidden_size, self.num_relations, self.hidden_size),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name=\"bilinear_W\"\n",
        "        )\n",
        "        # Bias vector of shape (num_relations,)\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(self.num_relations,),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            name=\"bilinear_bias\"\n",
        "        )\n",
        "        super(Bilinear, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        head, tail = inputs  # Both are (batch, hidden_size)\n",
        "        # Use einsum to compute: for each sample, head^T * W * tail.\n",
        "        # Equation: 'bi,irk,bj->br' where:\n",
        "        #   b: batch, i: hidden dimension from head, j: hidden dimension from tail, r: relation index.\n",
        "        logits = tf.einsum('bi,irk,bj->br', head, self.W, tail)\n",
        "        logits = logits + self.bias\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmOXaQT9mjMc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the relation extraction model as a subclass of tf.keras.Model.\n",
        "class RelationExtractionHead(tf.keras.Model):\n",
        "    def __init__(self, hidden_size, num_relations):\n",
        "        super(RelationExtractionHead, self).__init__()\n",
        "        self.head_proj = tf.keras.layers.Dense(hidden_size, name=\"head_proj\")\n",
        "        self.tail_proj = tf.keras.layers.Dense(hidden_size, name=\"tail_proj\")\n",
        "        self.bilinear = Bilinear(hidden_size, num_relations)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        h_i, h_j = inputs  # both are (batch, hidden_size)\n",
        "        head = self.head_proj(h_i)\n",
        "        tail = self.tail_proj(h_j)\n",
        "        logits = self.bilinear((head, tail))  # (batch, num_relations)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3ISuPinMAi-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "num_relations = 5                                         # Number of relation classes.\n",
        "num_epochs = 1                                            # Number of training epochs.\n",
        "learning_rate = 1e-4                                      # Learning rate.\n",
        "num_train_samples = len(train_dataset_hf)\n",
        "sample_base = int(np.ceil(num_train_samples / num_epochs))\n",
        "sample_range_min = int(sample_base * 0.95)\n",
        "sample_range_max = int(sample_base * 1.10)\n",
        "\n",
        "model_tf = RelationExtractionHead(hidden_size=hidden_size, num_relations=num_relations)\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "num_samples = len(train_dataset_hf)\n",
        "total_batches = int(np.ceil(num_samples / batch_size))\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    samples_this_epoch = random.randint(sample_range_min, sample_range_max)\n",
        "    total_batches = int(np.ceil(samples_this_epoch / batch_size))\n",
        "    epoch_dataset = (\n",
        "        train_dataset.unbatch()\n",
        "        .shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
        "        .take(samples_this_epoch)\n",
        "        .batch(batch_size)\n",
        "    )\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    batches = 0\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, sampling {samples_this_epoch} examples...\")\n",
        "    epoch_loss = []\n",
        "    for batch, (h_i_batch, h_j_batch, labels_batch, keys_batch) in enumerate(train_dataset.take(samples_this_epoch)):\n",
        "        if len(h_i_batch.shape) == 3:\n",
        "            h_i_batch = tf.reduce_mean(h_i_batch, axis=1)\n",
        "            h_j_batch = tf.reduce_mean(h_j_batch, axis=1)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model_tf((h_i_batch, h_j_batch), training=True)\n",
        "            loss = loss_fn(labels_batch, logits)\n",
        "        grads = tape.gradient(loss, model_tf.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model_tf.trainable_variables))\n",
        "        epoch_loss.append(loss.numpy())\n",
        "\n",
        "    avg_loss = np.mean(epoch_loss)\n",
        "    loss_history.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, num_epochs+1), loss_history, marker='o', linestyle='-')\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNs8MqOWKE5x"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "\n",
        "class RelationModelTesterTF:\n",
        "    def __init__(self, model, dataset, batch_size=32, num_samples=None):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.num_samples = num_samples or sum(1 for _ in dataset)\n",
        "        self.total_batches = int(np.ceil(self.num_samples / batch_size))\n",
        "\n",
        "    def evaluate(self):\n",
        "        all_predictions = []\n",
        "        all_ground_truths = []\n",
        "        all_probabilities = []\n",
        "        all_keys = []\n",
        "        all_h_i = []\n",
        "        all_h_j = []\n",
        "\n",
        "        progress_bar = tqdm(self.dataset, desc=\"Evaluating\", unit=\"batch\", total=self.total_batches)\n",
        "\n",
        "        for h_i_batch, h_j_batch, labels_batch, keys_batch in progress_bar:\n",
        "            # Pool if necessary\n",
        "            if len(h_i_batch.shape) == 3:\n",
        "\n",
        "                h_j_pooled = tf.reduce_mean(h_j_batch, axis=1)\n",
        "            else:\n",
        "                h_i_pooled = h_i_batch\n",
        "                h_j_pooled = h_j_batch\n",
        "\n",
        "            logits = self.model((h_i_pooled, h_j_pooled), training=False)\n",
        "            probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "            predictions = tf.argmax(probabilities, axis=-1)\n",
        "\n",
        "            all_predictions.extend(predictions.numpy())\n",
        "            all_ground_truths.extend(labels_batch.numpy())\n",
        "            all_probabilities.extend(probabilities.numpy())\n",
        "            all_keys.extend(keys_batch.numpy())\n",
        "            all_h_i.extend(h_i_pooled.numpy())\n",
        "            all_h_j.extend(h_j_pooled.numpy())\n",
        "\n",
        "        accuracy = np.mean(np.array(all_predictions) == np.array(all_ground_truths))\n",
        "        return {\n",
        "            \"predictions\": all_predictions,\n",
        "            \"ground_truth\": all_ground_truths,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"probabilities\": all_probabilities,\n",
        "            \"keys\": all_keys,\n",
        "            \"h_i\": all_h_i,\n",
        "            \"h_j\": all_h_j\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jlkzukx1UbLB"
      },
      "outputs": [],
      "source": [
        "# Convert the test split to a stable, properly batched tf.data.Dataset\n",
        "# Now pass the test dataset to the tester\n",
        "tester = RelationModelTesterTF(model_tf, test_dataset, batch_size=32, num_samples=len(test_dataset_hf))\n",
        "results = tester.evaluate()\n",
        "\n",
        "print(\"Test Accuracy: {:.2f}%\".format(results[\"accuracy\"] * 100))\n",
        "print(\"Example Predictions (first 10 samples):\", results[\"predictions\"][:10])\n",
        "\n",
        "# Print out predictions and labels\n",
        "print(\"Sample-wise Predictions:\")\n",
        "for i in range(min(10, len(results[\"predictions\"]))):\n",
        "    print(f\"\\nSample {i} (Key: {results['keys'][i]})\")\n",
        "    print(\"True Label      :\", results[\"ground_truth\"][i])\n",
        "    print(\"Predicted Label :\", results[\"predictions\"][i])\n",
        "    print(\"Probabilities   :\", np.round(results[\"probabilities\"][i], 4))\n",
        "    print(\"h_i (first 5)   :\", np.round(results[\"h_i\"][i][:5], 4))\n",
        "    print(\"h_j (first 5)   :\", np.round(results[\"h_j\"][i][:5], 4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import load_from_disk\n",
        "from collections import defaultdict\n",
        "from data_util import DataUtil  # your utility class\n",
        "\n",
        "# === CONFIG ===\n",
        "# OCR JSON folder (mounted under MyDrive)\n",
        "OCR_JSON_DIR = '/content/drive/MyDrive/KVP10K-dataset/kvp10k/test/ocrs'\n",
        "\n",
        "# map integer label → relation name\n",
        "ID2REL = {\n",
        "    0: 'no_relation',\n",
        "    1: 'parent_identifier',\n",
        "    2: 'invoice_date',\n",
        "    3: 'order_number',\n",
        "    4: 'sum',\n",
        "    5: 'customer_name',\n",
        "}\n",
        "\n",
        "# --- RUN INFERENCE ---\n",
        "# assume you already instantiated and trained:\n",
        "#   model_tf  = RelationExtractionHead(...)\n",
        "#   tester    = RelationModelTesterTF(model_tf, test_dataset, batch_size=32)\n",
        "results = tester.evaluate()\n",
        "\n",
        "# --- AGGREGATE TEXTUAL KEY/VALUE PAIRS ---\n",
        "matches = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "for example, pred_id, prob_dist in zip(\n",
        "        test_dataset_hf,\n",
        "        results['predictions'],\n",
        "        results['probabilities']\n",
        "    ):\n",
        "    doc_id   = example['doc_id']\n",
        "    head_idx = example['head_idx']\n",
        "    tail_idx = example['tail_idx']\n",
        "    rel_name = ID2REL[pred_id]\n",
        "    score    = round(float(prob_dist[pred_id]), 4)\n",
        "\n",
        "    # Load the OCR JSON and extract text lines\n",
        "    ocr_json = json.load(open(os.path.join(OCR_JSON_DIR, f\"{doc_id}.json\"), encoding='utf-8'))\n",
        "    lines    = ocr_json['form']\n",
        "    key_text   = lines[head_idx]['text']\n",
        "    value_text = lines[tail_idx]['text']\n",
        "\n",
        "    matches[doc_id][rel_name].append({\n",
        "        'key': key_text,\n",
        "        'value': value_text,\n",
        "        'score': score,\n",
        "    })\n",
        "\n",
        "# --- PRINT TOP-SCORING PAIR PER DOC ---\n",
        "for doc_id, rels in matches.items():\n",
        "    print(f\"\\n📄 Document: {doc_id}\")\n",
        "    for rel, items in rels.items():\n",
        "        best = max(items, key=lambda x: x['score'])\n",
        "        print(f\"{rel}: {best['value']}   \"\n",
        "              f\"(key='{best['key']}', score={best['score']})\")\n"
      ],
      "metadata": {
        "id": "Me26SiAvSquw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}